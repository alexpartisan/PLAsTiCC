{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger():\n",
    "    logger_ = logging.getLogger('main')\n",
    "    logger_.setLevel(logging.DEBUG)\n",
    "    fh = logging.FileHandler('simple_lightgbm.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    # add the handlers to the logger\n",
    "    logger_.addHandler(fh)\n",
    "    logger_.addHandler(ch)\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    return logging.getLogger('main')\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    if len(np.unique(y_true)) == 14:\n",
    "        classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "        class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "#     if len(np.unique(y_true)) > 14:\n",
    "#         classes.append(99)\n",
    "#         class_weight[99] = 2\n",
    "    \n",
    "    # Galaxy Case\n",
    "    if len(np.unique(y_true)) == 5:\n",
    "        classes = [6, 16, 53, 65, 92]\n",
    "        class_weight = {6: 1, 16: 1, 53: 1, 65: 1, 92: 1}\n",
    "        \n",
    "    # Out of Galaxy Case\n",
    "    if len(np.unique(y_true)) == 9:\n",
    "        classes = [15, 42, 52, 62, 64, 67, 88, 90, 95]\n",
    "        class_weight = {15: 2, 42: 1, 52: 1, 62: 1, 64: 2, 67: 1, 88: 1, 90: 1, 95: 1}\n",
    "        \n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    if len(np.unique(y_true)) == 14:\n",
    "        classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "        class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "#     if len(np.unique(y_true)) > 14:\n",
    "#         classes.append(99)\n",
    "#         class_weight[99] = 2\n",
    "        \n",
    "    # Galaxy Case\n",
    "    if len(np.unique(y_true)) == 5:\n",
    "        classes = [6, 16, 53, 65, 92]\n",
    "        class_weight = {6: 1, 16: 1, 53: 1, 65: 1, 92: 1}\n",
    "        \n",
    "    # Out of Galaxy Case\n",
    "    if len(np.unique(y_true)) == 9:\n",
    "        classes = [15, 42, 52, 62, 64, 67, 88, 90, 95]\n",
    "        class_weight = {15: 2, 42: 1, 52: 1, 62: 1, 64: 2, 67: 1, 88: 1, 90: 1, 95: 1}    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def predict_chunk(df_, clfs_, meta_, features, train_mean):\n",
    "\n",
    "    df_['flux_ratio_sq'] = np.power(df_['flux'] / df_['flux_err'], 2.0)\n",
    "    df_['flux_by_flux_ratio_sq'] = df_['flux'] * df_['flux_ratio_sq']\n",
    "\n",
    "    # Group by object id\n",
    "    aggs = get_aggregations()\n",
    "\n",
    "    aggs = get_aggregations()\n",
    "    aggs['flux_ratio_sq'] = ['sum']\n",
    "    aggs['flux_by_flux_ratio_sq'] = ['sum']\n",
    "\n",
    "    new_columns = get_new_columns(aggs)\n",
    "\n",
    "    agg_ = df_.groupby('object_id').agg(aggs)\n",
    "    agg_.columns = new_columns\n",
    "\n",
    "    agg_ = add_features_to_agg(df=agg_)\n",
    "\n",
    "    # Merge with meta data\n",
    "    full_test = agg_.reset_index().merge(\n",
    "        right=meta_,\n",
    "        how='left',\n",
    "        on='object_id'\n",
    "    )\n",
    "\n",
    "    full_test = full_test.fillna(train_mean)\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "\n",
    "    print(preds_df_['class_99'].mean())\n",
    "\n",
    "    del agg_, full_test, preds_\n",
    "    gc.collect()\n",
    "\n",
    "    return preds_df_\n",
    "\n",
    "\n",
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 24))\n",
    "    sns.barplot(x='gain', y='feature', data=importances_.sort_values('mean_gain', ascending=False))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'../feat/importances_{}.png'.format(datetime.datetime.now().strftime('%m%d_%H%M')))\n",
    "    importances_.sort_values('mean_gain', ascending=False).to_csv(r'../feat/feat_rank_{}.csv'.format(datetime.datetime.now().strftime('%m%d_%H%M')), index=False)\n",
    "\n",
    "\n",
    "def train_classifiers(full_train=None, y=None):\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 14,\n",
    "        'metric': 'multi_logloss',\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': .9,\n",
    "        'colsample_bytree': .7,\n",
    "        'reg_alpha': .01,\n",
    "        'reg_lambda': .01,\n",
    "        'min_split_gain': 0.01,\n",
    "        'min_child_weight': 10,\n",
    "        'n_estimators': 1000,\n",
    "        'silent': -1,\n",
    "        'verbose': -1,\n",
    "        'max_depth': 3\n",
    "    }\n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        get_logger().info(multi_weighted_logloss(val_y, clf.predict_proba(val_x, num_iteration=clf.best_iteration_)))\n",
    "\n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df['feature'] = full_train.columns\n",
    "        imp_df['gain'] = clf.feature_importances_\n",
    "        imp_df['fold'] = fold_ + 1\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "        clfs.append(clf)\n",
    "\n",
    "    get_logger().info('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "\n",
    "    return clfs, importances, oof_preds\n",
    "\n",
    "\n",
    "def get_aggregations():\n",
    "    return {\n",
    "        'mjd': ['min', 'max', 'size'],\n",
    "        'passband': ['mean', 'std', 'var'],  # ''min', 'max', 'mean', 'median', 'std'],\n",
    "#         'flux': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'detected': ['mean'],  # ''min', 'max', 'mean', 'median', 'std'],\n",
    "    }\n",
    "\n",
    "\n",
    "def get_new_columns(aggs):\n",
    "    return [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "\n",
    "def add_features_to_agg(df):\n",
    "    df['mjd_diff'] = df['mjd_max'] - df['mjd_min']\n",
    "    df['flux_diff'] = df['flux_max'] - df['flux_min']\n",
    "    df['flux_dif2'] = (df['flux_max'] - df['flux_min']) / df['flux_mean']\n",
    "    df['flux_w_mean'] = df['flux_by_flux_ratio_sq_sum'] / df['flux_ratio_sq_sum']\n",
    "    df['flux_dif3'] = (df['flux_max'] - df['flux_min']) / df['flux_w_mean']\n",
    "\n",
    "\n",
    "\n",
    "    del df['mjd_max'], df['mjd_min']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>mjd</th>\n",
       "      <th>passband</th>\n",
       "      <th>flux</th>\n",
       "      <th>flux_err</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4229</td>\n",
       "      <td>2</td>\n",
       "      <td>-544.810303</td>\n",
       "      <td>3.622952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4306</td>\n",
       "      <td>1</td>\n",
       "      <td>-816.434326</td>\n",
       "      <td>5.553370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4383</td>\n",
       "      <td>3</td>\n",
       "      <td>-471.385529</td>\n",
       "      <td>3.801213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4450</td>\n",
       "      <td>4</td>\n",
       "      <td>-388.984985</td>\n",
       "      <td>11.395031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>615</td>\n",
       "      <td>59752.4070</td>\n",
       "      <td>2</td>\n",
       "      <td>-681.858887</td>\n",
       "      <td>4.041204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id         mjd  passband        flux   flux_err  detected\n",
       "0        615  59750.4229         2 -544.810303   3.622952         1\n",
       "1        615  59750.4306         1 -816.434326   5.553370         1\n",
       "2        615  59750.4383         3 -471.385529   3.801213         1\n",
       "3        615  59750.4450         4 -388.984985  11.395031         1\n",
       "4        615  59752.4070         2 -681.858887   4.041204         1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/training_set.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['object_id', 'mjd', 'passband', 'flux', 'flux_err', 'detected'], dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.421705e+06\n",
       "mean     6.017921e+04\n",
       "std      3.092379e+02\n",
       "min      5.958003e+04\n",
       "25%      5.989905e+04\n",
       "50%      6.019331e+04\n",
       "75%      6.048722e+04\n",
       "max      6.067436e+04\n",
       "Name: mjd, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.mjd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_endpoints(df):\n",
    "    t_min = df.mjd.min()\n",
    "    t_max = df.mjd.max()\n",
    "    print('Min and Max MJD time : {}, {}'.format(t_min, t_max))\n",
    "    return t_min, t_max\n",
    "\n",
    "def add_feats_within_time_interval(int_n, t_min, t_max, df, db):\n",
    "    print('Number of Intervals :', int_n)\n",
    "    int_dur = (t_max - t_min)/int_n\n",
    "    for i in range(int_n):\n",
    "        \n",
    "        db_fil = db[(db.mjd>=(t_min+i*int_dur))&(db.mjd<(t_min + (i+1)*int_dur))][['object_id','flux','passband']]\n",
    "        print('Interval #{}, record quantity: {}'.format(i+1, db_fil.shape[0]))\n",
    "        \n",
    "        # interval_#_flux_？\n",
    "        stats = db_fil.groupby('object_id', as_index=False)['flux'].agg({'interval_{}_flux_mean'.format(i+1):'mean',\n",
    "                                                                          'interval_{}_flux_std'.format(i+1):'std',\n",
    "                                                                          'interval_{}_flux_min'.format(i+1):'min',\n",
    "                                                                          'interval_{}_flux_max'.format(i+1):'max',\n",
    "                                                                         'interval_{}_flux_skew'.format(i+1):'skew'})\n",
    "        print('New features added: ',stats.columns.tolist())\n",
    "        df = df.merge(stats, on='object_id', how='left')\n",
    "        \n",
    "        \n",
    "        # interval_#_band_#_flux_？\n",
    "#         stats = db_fil.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "#         stats.columns = ['interval_{}_band_{}_flux_skew'.format(i+1, str(col)) for col in stats.columns.tolist()]\n",
    "#         print('Feats added:',stats.columns.tolist())\n",
    "#         stats['object_id'] = stats.index    \n",
    "#         df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "        \n",
    "                \n",
    "    # interval_#_flux_？ 互相做差\n",
    "#     for key in ['max', 'min', 'mean']:\n",
    "#     for key in ['max']:\n",
    "#         key_cols = ['interval_{}_flux_{}'.format(i, key) for i in range(1, int_n+1)]\n",
    "#         for col in key_cols:\n",
    "#             subtract_cols = [col_ for col_ in key_cols if col_ < col]\n",
    "#             for sub_col in subtract_cols:\n",
    "#                 df['{}_minus_{}'.format(col, sub_col)] = df[col] - df[sub_col]\n",
    "#                 print('Feature added:', '{}_minus_{}'.format(col, sub_col))\n",
    "        \n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>ra</th>\n",
       "      <th>decl</th>\n",
       "      <th>gal_l</th>\n",
       "      <th>gal_b</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hostgal_specz</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>distmod</th>\n",
       "      <th>mwebv</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>349.046051</td>\n",
       "      <td>-61.943836</td>\n",
       "      <td>320.796530</td>\n",
       "      <td>-51.753706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>713</td>\n",
       "      <td>53.085938</td>\n",
       "      <td>-27.784405</td>\n",
       "      <td>223.525509</td>\n",
       "      <td>-54.460748</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8181</td>\n",
       "      <td>1.6267</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>45.4063</td>\n",
       "      <td>0.007</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730</td>\n",
       "      <td>33.574219</td>\n",
       "      <td>-6.579593</td>\n",
       "      <td>170.455585</td>\n",
       "      <td>-61.548219</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>40.2561</td>\n",
       "      <td>0.021</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>745</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>-45.586655</td>\n",
       "      <td>328.254458</td>\n",
       "      <td>-68.969298</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3037</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>1.1523</td>\n",
       "      <td>40.7951</td>\n",
       "      <td>0.007</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1124</td>\n",
       "      <td>352.711273</td>\n",
       "      <td>-63.823658</td>\n",
       "      <td>316.922299</td>\n",
       "      <td>-51.059403</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>40.4166</td>\n",
       "      <td>0.024</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id          ra       decl       gal_l      gal_b  ddf  \\\n",
       "0        615  349.046051 -61.943836  320.796530 -51.753706    1   \n",
       "1        713   53.085938 -27.784405  223.525509 -54.460748    1   \n",
       "2        730   33.574219  -6.579593  170.455585 -61.548219    1   \n",
       "3        745    0.189873 -45.586655  328.254458 -68.969298    1   \n",
       "4       1124  352.711273 -63.823658  316.922299 -51.059403    1   \n",
       "\n",
       "   hostgal_specz  hostgal_photoz  hostgal_photoz_err  distmod  mwebv  target  \n",
       "0         0.0000          0.0000              0.0000      NaN  0.017      92  \n",
       "1         1.8181          1.6267              0.2552  45.4063  0.007      88  \n",
       "2         0.2320          0.2262              0.0157  40.2561  0.021      42  \n",
       "3         0.3037          0.2813              1.1523  40.7951  0.007      90  \n",
       "4         0.1934          0.2415              0.0176  40.4166  0.024      90  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train = pd.read_csv('../input/training_set_metadata.csv')\n",
    "meta_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train[~meta_train.distmod.isnull()].target.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_band_feats(df, db):\n",
    "    \n",
    "    \n",
    "### 均值\n",
    "    print('Adding feats for the flux mean per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_mean' for col in stats.columns.tolist()]\n",
    "    \n",
    "    # band_#_flux_mean互相减去\n",
    "    mean_cols = stats.columns.tolist()\n",
    "    for col in mean_cols:\n",
    "        subtract_cols = [col_ for col_ in mean_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "      \n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "### 标准差    \n",
    "    print('Adding feats for the flux std per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].std().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_std' for col in stats.columns.tolist()]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "\n",
    "    \n",
    "### 偏度    \n",
    "    print('Adding feats for the flux skew per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_skew' for col in stats.columns.tolist()]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "### 最大值\n",
    "    print('Adding feats for the flux max per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].max().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_max' for col in stats.columns.tolist()]\n",
    "    # band_#_flux_max互相减去\n",
    "    max_cols = stats.columns.tolist()\n",
    "    for col in max_cols:\n",
    "        subtract_cols = [col_ for col_ in max_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "            \n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "### 最小值    \n",
    "    print('Adding feats for the flux min per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].min().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_min' for col in stats.columns.tolist()]\n",
    "    # band_#_flux_min互相做差\n",
    "    min_cols = stats.columns.tolist()\n",
    "    for col in min_cols:\n",
    "        subtract_cols = [col_ for col_ in min_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# ### ......  Mean\n",
    "#     print('Adding feats for the flux_err mean per band...')\n",
    "#     stats = db.groupby(['object_id','passband'])['flux_err'].mean().unstack()\n",
    "#     stats.columns = ['band_' + str(col) + '_flux_err_mean' for col in stats.columns.tolist()]      \n",
    "#     print('Feats added:',stats.columns.tolist())\n",
    "#     stats['object_id'] = stats.index    \n",
    "#     df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### 遍历band_list计算 \n",
    "    print('Adding feats for the flux (max-min)/mean per band...')\n",
    "    for band_n in range(6):\n",
    "        df['band_' + str(band_n) + '_flux_diff1'] = df['band_' + str(band_n) + '_flux_max'] - df['band_' + str(band_n) + '_flux_min']\n",
    "        df['band_' + str(band_n) + '_flux_diff2'] = df['band_' + str(band_n) + '_flux_diff1']/df['band_' + str(band_n) + '_flux_mean']\n",
    "        print('Feature added: band_' + str(band_n) + '_flux_diff2')\n",
    "        \n",
    "#         df['band_' + str(band_n) + '_flux_err_ratio'] = df['band_' + str(band_n) + '_flux_err_mean']/df['band_' + str(band_n) + '_flux_mean']\n",
    "#         print('Feature added: band_' + str(band_n) + '_flux_err_ratio')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "#     print('Adding feats for the flux mean per band...')\n",
    "#     stats = db.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "#     stats['object_id'] = stats.index\n",
    "#     stats.columns = [str(col) + '_mean' for col in stats.columns.tolist()]\n",
    "#     df = df.merge(db, on='object_id', how='left').fillna(0)\n",
    "#     print('Feats added:',stats.columns.tolist())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_ratio_col(df):\n",
    "#     df['hostgal_photoz_ratio'] = df['hostgal_photoz_err']/df['hostgal_photoz']\n",
    "#     df['hostgal_photoz_ratio'].fillna(0, inplace=True)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv('../input/training_set.csv')\n",
    "    \n",
    "    # 增加按MJD划分时间统计得到的特征\n",
    "    mjd_min, mjd_max = get_time_endpoints(train)\n",
    "    # 增加按MJD划分时间统计得到的特征\n",
    "    \n",
    "    \n",
    "    train['flux_ratio_sq'] = np.power(train['flux'] / train['flux_err'], 2.0)\n",
    "    train['flux_by_flux_ratio_sq'] = train['flux'] * train['flux_ratio_sq']\n",
    "\n",
    "\n",
    "    aggs = get_aggregations()\n",
    "    aggs['flux_ratio_sq'] = ['sum']\n",
    "    aggs['flux_by_flux_ratio_sq'] = ['sum']\n",
    "\n",
    "    \n",
    "    agg_train = train.groupby('object_id').agg(aggs)\n",
    "    new_columns = get_new_columns(aggs)\n",
    "    agg_train.columns = new_columns\n",
    "\n",
    "    agg_train = add_features_to_agg(df=agg_train)\n",
    "\n",
    "    meta_train = pd.read_csv('../input/training_set_metadata.csv')\n",
    "\n",
    "    \n",
    "    # 增加是否在银河系的特征\n",
    "    meta_train.distmod.fillna(0,inplace=True)\n",
    "    meta_train['in_galaxy'] = 0\n",
    "    meta_train.loc[(meta_train.distmod == 0), 'in_galaxy'] = 1\n",
    "    # 增加是否在银河系的特征\n",
    "\n",
    "    full_train = agg_train.reset_index().merge(\n",
    "        right=meta_train,\n",
    "        how='outer',\n",
    "        on='object_id'\n",
    "    )\n",
    "    \n",
    "    del full_train['hostgal_specz']\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     y = full_train['target']\n",
    "#     del full_train['target']\n",
    "\n",
    "      \n",
    "    # TO DO!!!!! Seperately calculated within objects in galaxy\n",
    "    train_mean = full_train.mean(axis=0)\n",
    "    full_train.fillna(train_mean, inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 增加 mean_flux_per_band\n",
    "    full_train = add_band_feats(full_train, train)\n",
    "    # 增加 mean_flux_per_band\n",
    "    \n",
    "    \n",
    "    # 增加按MJD划分时间统计得到的特征\n",
    "    full_train = add_feats_within_time_interval(6, mjd_min, mjd_max, full_train, train)\n",
    "    # 增加按MJD划分时间统计得到的特征 \n",
    "    \n",
    "#     # 增加ratio特征\n",
    "#     full_train = add_ratio_col(full_train)\n",
    "#     # 增加ratio特征\n",
    "\n",
    "    objects = full_train['object_id'].unique().tolist()\n",
    "    print('Number of objects :',len(objects))\n",
    "   \n",
    "    full_train_in_gal = full_train[full_train['in_galaxy']==1]\n",
    "    objects_in_gal = full_train_in_gal['object_id'].unique().tolist()\n",
    "    print('Number of objects in galaxy :',len(objects_in_gal))\n",
    "    \n",
    "    full_train_out_gal = full_train[full_train['in_galaxy']==0]\n",
    "    objects_out_gal = full_train_out_gal['object_id'].unique().tolist()\n",
    "    print('Number of objects out of galaxy :',len(objects_out_gal))\n",
    "    print('Just to check, sum of objects :', len(objects_in_gal) + len(objects_out_gal))\n",
    "    \n",
    "    del train\n",
    "    \n",
    "    y_list = []\n",
    "    preds_list = []\n",
    "    for df in [full_train_in_gal, full_train_out_gal]:\n",
    "        del df['object_id']\n",
    "        y = df['target']\n",
    "        y_list.append(y)\n",
    "        del df['target']\n",
    "        gc.collect()\n",
    "        get_logger().info(df.columns)\n",
    "        clfs, importances, preds = train_classifiers(df, y)\n",
    "        preds_list.append(preds)\n",
    "    \n",
    "    print('Let us check the logloss when objects are trained separately...')\n",
    "    all_y = pd.concat(y_list, ignore_index=True)\n",
    "\n",
    "    preds_in_gal = preds_list[0]\n",
    "    preds_out_gal = preds_list[1]\n",
    "    in_classes = [6, 16, 53, 65, 92]\n",
    "    out_classes = [15, 42, 52, 62, 64, 67, 88, 90, 95]\n",
    "    print(preds_in_gal)\n",
    "    preds_in_gal[out_classes] = 0\n",
    "    preds_out_gal[in_classes] = 0\n",
    "    print(preds_in_gal)\n",
    "    \n",
    "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    get_logger().info('Separate MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=all_y, y_preds=all_preds))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     del full_train['object_id']\n",
    "        \n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "        \n",
    "#     get_logger().info(full_train.columns)\n",
    "#     clfs, importances = train_classifiers(full_train, y)\n",
    "\n",
    "#     save_importances(importances_=importances)\n",
    "\n",
    "#     meta_test = pd.read_csv('../input/test_set_metadata.csv')\n",
    "\n",
    "#     import time\n",
    "\n",
    "#     start = time.time()\n",
    "#     chunks = 5000000\n",
    "#     remain_df = None\n",
    "\n",
    "#     for i_c, df in enumerate(pd.read_csv('../input/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "#         # Check object_ids\n",
    "#         # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "#         unique_ids = np.unique(df['object_id'])\n",
    "#         new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "\n",
    "#         if remain_df is None:\n",
    "#             df = df.loc[df['object_id'].isin(unique_ids[:-1])].copy()\n",
    "#         else:\n",
    "#             df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "\n",
    "#         # Create remaining samples df\n",
    "#         remain_df = new_remain_df\n",
    "\n",
    "#         preds_df = predict_chunk(df_=df,\n",
    "#                                  clfs_=clfs,\n",
    "#                                  meta_=meta_test,\n",
    "#                                  features=full_train.columns,\n",
    "#                                  train_mean=train_mean)\n",
    "\n",
    "#         if i_c == 0:\n",
    "#             preds_df.to_csv('predictions_v3.csv', header=True, index=False, float_format='%.6f')\n",
    "#         else:\n",
    "#             preds_df.to_csv('predictions_v3.csv', header=False, mode='a', index=False, float_format='%.6f')\n",
    "\n",
    "#         del preds_df\n",
    "#         gc.collect()\n",
    "\n",
    "#         if (i_c + 1) % 10 == 0:\n",
    "#             get_logger().info('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "#             print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "\n",
    "#     # Compute last object in remain_df\n",
    "\n",
    "#     preds_df = predict_chunk(df_=remain_df,\n",
    "#                              clfs_=clfs,\n",
    "#                              meta_=meta_test,\n",
    "#                              features=full_train.columns,\n",
    "#                              train_mean=train_mean)\n",
    "\n",
    "#     preds_df.to_csv('predictions_v3.csv', header=False, mode='a', index=False, float_format='%.6f')\n",
    "\n",
    "#     z = pd.read_csv('predictions_v3.csv')\n",
    "\n",
    "#     z = z.groupby('object_id').mean()\n",
    "\n",
    "#     z.to_csv('single_predictions_v3.csv', index=True, float_format='%.6f')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max MJD time : 59580.0343, 60674.363\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n",
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:36: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Interval #1, record quantity: 107465\n",
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_std', 'interval_1_flux_min', 'interval_1_flux_max', 'interval_1_flux_skew']\n",
      "Interval #2, record quantity: 327857\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_std', 'interval_2_flux_min', 'interval_2_flux_max', 'interval_2_flux_skew']\n",
      "Interval #3, record quantity: 183160\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_std', 'interval_3_flux_min', 'interval_3_flux_max', 'interval_3_flux_skew']\n",
      "Interval #4, record quantity: 289238\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_std', 'interval_4_flux_min', 'interval_4_flux_max', 'interval_4_flux_skew']\n",
      "Interval #5, record quantity: 166471\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min', 'interval_5_flux_max', 'interval_5_flux_skew']\n",
      "Interval #6, record quantity: 347513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:44:58,243:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max', 'interval_6_flux_skew']\n",
      "Number of objects : 7848\n",
      "Number of objects in galaxy : 2325\n",
      "Number of objects out of galaxy : 5523\n",
      "Just to check, sum of objects : 7848\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.173063\ttraining's wloss: 0.303189\tvalid_1's multi_logloss: 0.20991\tvalid_1's wloss: 0.355913\n",
      "[200]\ttraining's multi_logloss: 0.0566046\ttraining's wloss: 0.124918\tvalid_1's multi_logloss: 0.10271\tvalid_1's wloss: 0.205894\n",
      "[300]\ttraining's multi_logloss: 0.0321509\ttraining's wloss: 0.0772599\tvalid_1's multi_logloss: 0.0855127\tvalid_1's wloss: 0.17653\n",
      "[400]\ttraining's multi_logloss: 0.023847\ttraining's wloss: 0.0602653\tvalid_1's multi_logloss: 0.0782606\tvalid_1's wloss: 0.163907\n",
      "[500]\ttraining's multi_logloss: 0.0202142\ttraining's wloss: 0.0523872\tvalid_1's multi_logloss: 0.074765\tvalid_1's wloss: 0.157237\n",
      "[600]\ttraining's multi_logloss: 0.0179814\ttraining's wloss: 0.0477012\tvalid_1's multi_logloss: 0.0724455\tvalid_1's wloss: 0.153812\n",
      "[700]\ttraining's multi_logloss: 0.0166546\ttraining's wloss: 0.044981\tvalid_1's multi_logloss: 0.0709237\tvalid_1's wloss: 0.151537\n",
      "[800]\ttraining's multi_logloss: 0.0156985\ttraining's wloss: 0.0428983\tvalid_1's multi_logloss: 0.0699528\tvalid_1's wloss: 0.149767\n",
      "[900]\ttraining's multi_logloss: 0.0149113\ttraining's wloss: 0.0411594\tvalid_1's multi_logloss: 0.0695352\tvalid_1's wloss: 0.148267\n",
      "Early stopping, best iteration is:\n",
      "[889]\ttraining's multi_logloss: 0.0149836\ttraining's wloss: 0.0412835\tvalid_1's multi_logloss: 0.0694994\tvalid_1's wloss: 0.14828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n",
      "[INFO]2018-10-26 17:45:05,293:main:0.14828040997767505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.166844\ttraining's wloss: 0.285222\tvalid_1's multi_logloss: 0.23818\tvalid_1's wloss: 0.595933\n",
      "[200]\ttraining's multi_logloss: 0.0514224\ttraining's wloss: 0.113096\tvalid_1's multi_logloss: 0.141797\tvalid_1's wloss: 0.497331\n",
      "[300]\ttraining's multi_logloss: 0.0291552\ttraining's wloss: 0.0732652\tvalid_1's multi_logloss: 0.128991\tvalid_1's wloss: 0.467294\n",
      "[400]\ttraining's multi_logloss: 0.0221696\ttraining's wloss: 0.0589562\tvalid_1's multi_logloss: 0.125734\tvalid_1's wloss: 0.454157\n",
      "[500]\ttraining's multi_logloss: 0.0189793\ttraining's wloss: 0.0516911\tvalid_1's multi_logloss: 0.124897\tvalid_1's wloss: 0.449284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n",
      "[INFO]2018-10-26 17:45:09,828:main:0.44880832801356496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[492]\ttraining's multi_logloss: 0.019194\ttraining's wloss: 0.0521968\tvalid_1's multi_logloss: 0.124786\tvalid_1's wloss: 0.448808\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.172738\ttraining's wloss: 0.302483\tvalid_1's multi_logloss: 0.231212\tvalid_1's wloss: 0.405422\n",
      "[200]\ttraining's multi_logloss: 0.0562276\ttraining's wloss: 0.123142\tvalid_1's multi_logloss: 0.123532\tvalid_1's wloss: 0.26932\n",
      "[300]\ttraining's multi_logloss: 0.0320568\ttraining's wloss: 0.0758915\tvalid_1's multi_logloss: 0.100832\tvalid_1's wloss: 0.236401\n",
      "[400]\ttraining's multi_logloss: 0.0237991\ttraining's wloss: 0.0593017\tvalid_1's multi_logloss: 0.0913512\tvalid_1's wloss: 0.221584\n",
      "[500]\ttraining's multi_logloss: 0.0202285\ttraining's wloss: 0.0520531\tvalid_1's multi_logloss: 0.0868926\tvalid_1's wloss: 0.216088\n",
      "[600]\ttraining's multi_logloss: 0.0179825\ttraining's wloss: 0.0474943\tvalid_1's multi_logloss: 0.0841861\tvalid_1's wloss: 0.213313\n",
      "[700]\ttraining's multi_logloss: 0.0166203\ttraining's wloss: 0.0445229\tvalid_1's multi_logloss: 0.0822533\tvalid_1's wloss: 0.211074\n",
      "[800]\ttraining's multi_logloss: 0.015631\ttraining's wloss: 0.0424559\tvalid_1's multi_logloss: 0.0803295\tvalid_1's wloss: 0.208146\n",
      "[900]\ttraining's multi_logloss: 0.0149264\ttraining's wloss: 0.0407749\tvalid_1's multi_logloss: 0.0791527\tvalid_1's wloss: 0.206425\n",
      "[1000]\ttraining's multi_logloss: 0.0143851\ttraining's wloss: 0.0393763\tvalid_1's multi_logloss: 0.0785641\tvalid_1's wloss: 0.205354\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's multi_logloss: 0.0143851\ttraining's wloss: 0.0393763\tvalid_1's multi_logloss: 0.0785641\tvalid_1's wloss: 0.205354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n",
      "[INFO]2018-10-26 17:45:17,307:main:0.20535380218064753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.177308\ttraining's wloss: 0.310638\tvalid_1's multi_logloss: 0.205153\tvalid_1's wloss: 0.405693\n",
      "[200]\ttraining's multi_logloss: 0.0570348\ttraining's wloss: 0.119543\tvalid_1's multi_logloss: 0.0939435\tvalid_1's wloss: 0.266677\n",
      "[300]\ttraining's multi_logloss: 0.0325936\ttraining's wloss: 0.0737848\tvalid_1's multi_logloss: 0.0731415\tvalid_1's wloss: 0.243378\n",
      "[400]\ttraining's multi_logloss: 0.0238836\ttraining's wloss: 0.057273\tvalid_1's multi_logloss: 0.066586\tvalid_1's wloss: 0.238053\n",
      "[500]\ttraining's multi_logloss: 0.0202344\ttraining's wloss: 0.050026\tvalid_1's multi_logloss: 0.0627294\tvalid_1's wloss: 0.22971\n",
      "[600]\ttraining's multi_logloss: 0.017935\ttraining's wloss: 0.0455869\tvalid_1's multi_logloss: 0.0597671\tvalid_1's wloss: 0.224173\n",
      "[700]\ttraining's multi_logloss: 0.0166219\ttraining's wloss: 0.0431149\tvalid_1's multi_logloss: 0.0583808\tvalid_1's wloss: 0.222794\n",
      "[800]\ttraining's multi_logloss: 0.0157024\ttraining's wloss: 0.0412314\tvalid_1's multi_logloss: 0.0573842\tvalid_1's wloss: 0.220828\n",
      "[900]\ttraining's multi_logloss: 0.0149216\ttraining's wloss: 0.039527\tvalid_1's multi_logloss: 0.0565192\tvalid_1's wloss: 0.219755\n",
      "[1000]\ttraining's multi_logloss: 0.0143979\ttraining's wloss: 0.0383173\tvalid_1's multi_logloss: 0.055769\tvalid_1's wloss: 0.218078\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's multi_logloss: 0.0143979\ttraining's wloss: 0.0383173\tvalid_1's multi_logloss: 0.055769\tvalid_1's wloss: 0.218078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n",
      "[INFO]2018-10-26 17:45:24,569:main:0.2180777036372446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.170151\ttraining's wloss: 0.299634\tvalid_1's multi_logloss: 0.233826\tvalid_1's wloss: 0.458547\n",
      "[200]\ttraining's multi_logloss: 0.0530274\ttraining's wloss: 0.120276\tvalid_1's multi_logloss: 0.131904\tvalid_1's wloss: 0.345125\n",
      "[300]\ttraining's multi_logloss: 0.0298529\ttraining's wloss: 0.0758331\tvalid_1's multi_logloss: 0.115984\tvalid_1's wloss: 0.321472\n",
      "[400]\ttraining's multi_logloss: 0.0222036\ttraining's wloss: 0.0597377\tvalid_1's multi_logloss: 0.110168\tvalid_1's wloss: 0.314644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,837:main:0.3109326607085269\n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's multi_logloss: 0.0191075\ttraining's wloss: 0.0532534\tvalid_1's multi_logloss: 0.108296\tvalid_1's wloss: 0.311381\n",
      "Early stopping, best iteration is:\n",
      "[455]\ttraining's multi_logloss: 0.0202868\ttraining's wloss: 0.0557481\tvalid_1's multi_logloss: 0.108766\tvalid_1's wloss: 0.310933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,858:main:MULTI WEIGHTED LOG LOSS : 0.26598 \n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n",
      "[INFO]2018-10-26 17:45:28,911:main:Index(['mjd_size', 'passband_mean', 'passband_std', 'passband_var', 'flux_min',\n",
      "       'flux_max', 'flux_mean', 'flux_median', 'flux_std', 'flux_skew',\n",
      "       ...\n",
      "       'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_5_flux_skew', 'interval_6_flux_mean',\n",
      "       'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max',\n",
      "       'interval_6_flux_skew'],\n",
      "      dtype='object', length=150)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.921516\ttraining's wloss: 1.15017\tvalid_1's multi_logloss: 1.04018\tvalid_1's wloss: 1.42094\n",
      "[200]\ttraining's multi_logloss: 0.720917\ttraining's wloss: 0.910335\tvalid_1's multi_logloss: 0.908458\tvalid_1's wloss: 1.31635\n",
      "[300]\ttraining's multi_logloss: 0.615887\ttraining's wloss: 0.753441\tvalid_1's multi_logloss: 0.866422\tvalid_1's wloss: 1.26983\n",
      "[400]\ttraining's multi_logloss: 0.540528\ttraining's wloss: 0.63473\tvalid_1's multi_logloss: 0.849522\tvalid_1's wloss: 1.25684\n",
      "[500]\ttraining's multi_logloss: 0.481316\ttraining's wloss: 0.545276\tvalid_1's multi_logloss: 0.839176\tvalid_1's wloss: 1.25149\n",
      "Early stopping, best iteration is:\n",
      "[547]\ttraining's multi_logloss: 0.456563\ttraining's wloss: 0.510182\tvalid_1's multi_logloss: 0.835059\tvalid_1's wloss: 1.24786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n",
      "[INFO]2018-10-26 17:45:43,458:main:1.247856044059452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.923772\ttraining's wloss: 1.15709\tvalid_1's multi_logloss: 1.02228\tvalid_1's wloss: 1.31843\n",
      "[200]\ttraining's multi_logloss: 0.717877\ttraining's wloss: 0.902927\tvalid_1's multi_logloss: 0.890371\tvalid_1's wloss: 1.21129\n",
      "[300]\ttraining's multi_logloss: 0.612546\ttraining's wloss: 0.737539\tvalid_1's multi_logloss: 0.850923\tvalid_1's wloss: 1.17773\n",
      "Early stopping, best iteration is:\n",
      "[328]\ttraining's multi_logloss: 0.589216\ttraining's wloss: 0.700928\tvalid_1's multi_logloss: 0.844312\tvalid_1's wloss: 1.17515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n",
      "[INFO]2018-10-26 17:45:53,086:main:1.1751465364010953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.913685\ttraining's wloss: 1.1532\tvalid_1's multi_logloss: 1.05442\tvalid_1's wloss: 1.35213\n",
      "[200]\ttraining's multi_logloss: 0.7078\ttraining's wloss: 0.901259\tvalid_1's multi_logloss: 0.93517\tvalid_1's wloss: 1.24505\n",
      "[300]\ttraining's multi_logloss: 0.60185\ttraining's wloss: 0.735854\tvalid_1's multi_logloss: 0.902871\tvalid_1's wloss: 1.21047\n",
      "[400]\ttraining's multi_logloss: 0.526683\ttraining's wloss: 0.615346\tvalid_1's multi_logloss: 0.887971\tvalid_1's wloss: 1.19958\n",
      "Early stopping, best iteration is:\n",
      "[390]\ttraining's multi_logloss: 0.533565\ttraining's wloss: 0.626079\tvalid_1's multi_logloss: 0.888571\tvalid_1's wloss: 1.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n",
      "[INFO]2018-10-26 17:46:04,318:main:1.198000494989074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.922594\ttraining's wloss: 1.1521\tvalid_1's multi_logloss: 1.02635\tvalid_1's wloss: 1.37573\n",
      "[200]\ttraining's multi_logloss: 0.718796\ttraining's wloss: 0.901069\tvalid_1's multi_logloss: 0.898117\tvalid_1's wloss: 1.28471\n",
      "[300]\ttraining's multi_logloss: 0.612144\ttraining's wloss: 0.737813\tvalid_1's multi_logloss: 0.855476\tvalid_1's wloss: 1.24615\n",
      "[400]\ttraining's multi_logloss: 0.537604\ttraining's wloss: 0.622339\tvalid_1's multi_logloss: 0.836696\tvalid_1's wloss: 1.23985\n",
      "[500]\ttraining's multi_logloss: 0.479493\ttraining's wloss: 0.536826\tvalid_1's multi_logloss: 0.827521\tvalid_1's wloss: 1.23781\n",
      "Early stopping, best iteration is:\n",
      "[464]\ttraining's multi_logloss: 0.498985\ttraining's wloss: 0.565371\tvalid_1's multi_logloss: 0.830317\tvalid_1's wloss: 1.23749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n",
      "[INFO]2018-10-26 17:46:16,327:main:1.2374936873479199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.9253\ttraining's wloss: 1.15564\tvalid_1's multi_logloss: 1.02136\tvalid_1's wloss: 1.37753\n",
      "[200]\ttraining's multi_logloss: 0.725477\ttraining's wloss: 0.912138\tvalid_1's multi_logloss: 0.893709\tvalid_1's wloss: 1.27375\n",
      "[300]\ttraining's multi_logloss: 0.620919\ttraining's wloss: 0.748793\tvalid_1's multi_logloss: 0.854586\tvalid_1's wloss: 1.23211\n",
      "[400]\ttraining's multi_logloss: 0.546618\ttraining's wloss: 0.629779\tvalid_1's multi_logloss: 0.839868\tvalid_1's wloss: 1.21994\n",
      "[500]\ttraining's multi_logloss: 0.48813\ttraining's wloss: 0.542624\tvalid_1's multi_logloss: 0.831018\tvalid_1's wloss: 1.2166\n",
      "Early stopping, best iteration is:\n",
      "[512]\ttraining's multi_logloss: 0.481882\ttraining's wloss: 0.533461\tvalid_1's multi_logloss: 0.829983\tvalid_1's wloss: 1.2156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,804:main:1.2156008320234841\n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[INFO]2018-10-26 17:46:28,823:main:MULTI WEIGHTED LOG LOSS : 1.21493 \n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n",
      "[ERROR]2018-10-26 17:46:28,849:main:Unexpected Exception Occured\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-641eff26fffe>\", line 4, in <module>\n",
      "    main()\n",
      "  File \"<ipython-input-71-562bc8201517>\", line 105, in main\n",
      "    all_preds = np.concatenate((preds_in_gal, preds_out_gal), axis=0)\n",
      "ValueError: all the input array dimensions except for the concatenation axis must match exactly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the logloss when objects are trained separately...\n",
      "[[6.06056631e-04 8.98046626e-03 9.37368349e-04 5.17867431e-04\n",
      "  9.88958241e-01]\n",
      " [2.59190612e-04 3.08585546e-04 2.31513767e-04 9.99075888e-01\n",
      "  1.24822029e-04]\n",
      " [1.45169491e-04 7.48674133e-03 2.66576104e-04 9.91978526e-01\n",
      "  1.22987430e-04]\n",
      " ...\n",
      " [3.69622977e-02 9.19472009e-01 7.39521507e-03 2.87742007e-02\n",
      "  7.39627723e-03]\n",
      " [2.48841160e-03 2.81507799e-04 3.31812427e-04 9.96468606e-01\n",
      "  4.29662438e-04]\n",
      " [9.63891255e-01 5.48330708e-04 2.87639271e-02 3.14884926e-03\n",
      "  3.64763771e-03]]\n",
      "[[6.06056631e-04 8.98046626e-03 9.37368349e-04 5.17867431e-04\n",
      "  9.88958241e-01]\n",
      " [2.59190612e-04 3.08585546e-04 2.31513767e-04 9.99075888e-01\n",
      "  1.24822029e-04]\n",
      " [1.45169491e-04 7.48674133e-03 2.66576104e-04 9.91978526e-01\n",
      "  1.22987430e-04]\n",
      " ...\n",
      " [3.69622977e-02 9.19472009e-01 7.39521507e-03 2.87742007e-02\n",
      "  7.39627723e-03]\n",
      " [2.48841160e-03 2.81507799e-04 3.31812427e-04 9.96468606e-01\n",
      "  4.29662438e-04]\n",
      " [9.63891255e-01 5.48330708e-04 2.87639271e-02 3.14884926e-03\n",
      "  3.64763771e-03]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-641eff26fffe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcreate_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mget_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unexpected Exception Occured'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-562bc8201517>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds_in_gal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mall_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds_in_gal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_out_gal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "gc.enable()\n",
    "create_logger()\n",
    "try:\n",
    "    main()\n",
    "except Exception:\n",
    "    get_logger().exception('Unexpected Exception Occured')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
