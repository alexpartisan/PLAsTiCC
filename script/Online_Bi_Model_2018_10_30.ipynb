{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature hostgal_specz is removed\n",
      "Dimension of merge data for MJD relevant data and META data  (1421705, 17)\n",
      "Number of objects in galaxy : 2325\n",
      "Number of objects out of galaxy : 5523\n",
      "Just to check, sum of objects : 7848\n",
      "Total number should be 7848\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (400574, 17) (1021131, 17)\n",
      "Dimension of aggregated data on flux features (7848, 18)\n",
      "Dimension of merge data for Object relevant data and META data (7848, 30)\n",
      "Number of objects in galaxy : 2325\n",
      "Number of objects out of galaxy : 5523\n",
      "Just to check, sum of objects : 7848\n",
      "Total number should be 7848\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (2325, 30) (5523, 30)\n",
      "Features extraction begins...\n",
      "In terms of that in the Galaxy...\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n",
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:312: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:321: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n",
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n",
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Min and Max MJD time : 59580.0347, 60674.363\n",
      "Interval #1, record quantity: 38277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:330: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:346: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:362: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_std', 'interval_1_flux_min', 'interval_1_flux_max', 'interval_1_flux_skew']\n",
      "Interval #2, record quantity: 82096\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_std', 'interval_2_flux_min', 'interval_2_flux_max', 'interval_2_flux_skew']\n",
      "Interval #3, record quantity: 61348\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_std', 'interval_3_flux_min', 'interval_3_flux_max', 'interval_3_flux_skew']\n",
      "Interval #4, record quantity: 73604\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_std', 'interval_4_flux_min', 'interval_4_flux_max', 'interval_4_flux_skew']\n",
      "Interval #5, record quantity: 56390\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min', 'interval_5_flux_max', 'interval_5_flux_skew']\n",
      "Interval #6, record quantity: 88858\n",
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max', 'interval_6_flux_skew']\n",
      "Feature added: interval_2_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_3_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_3_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_4_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_4_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_5_flux_max\n",
      "Feature added: interval_2_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_3_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_3_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_4_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_4_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_5_flux_min\n",
      "Feature added: interval_2_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_3_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_3_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_4_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_4_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_5_flux_mean\n",
      "Dimension of data after adding features relevant to time intervals (2325, 192)\n",
      "In terms of that out of the Galaxy...\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:528: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n",
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n",
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n",
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Min and Max MJD time : 59580.0343, 60674.3625\n",
      "Interval #1, record quantity: 69188\n",
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_min', 'interval_1_flux_max']\n",
      "Interval #2, record quantity: 245761\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_min', 'interval_2_flux_max']\n",
      "Interval #3, record quantity: 121812\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_min', 'interval_3_flux_max']\n",
      "Interval #4, record quantity: 215634\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_min', 'interval_4_flux_max']\n",
      "Interval #5, record quantity: 110081\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_min', 'interval_5_flux_max']\n",
      "Interval #6, record quantity: 258652\n",
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_min', 'interval_6_flux_max']\n",
      "Dimension of data after adding features relevant to time intervals (5523, 136)\n",
      "Training begins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:05:52,464:main:Index(['flux_min', 'flux_max', 'flux_mean', 'flux_median', 'flux_std',\n",
      "       'flux_skew', 'flux_err_min', 'flux_err_max', 'flux_err_mean',\n",
      "       'flux_err_median',\n",
      "       ...\n",
      "       'interval_4_flux_mean_minus_interval_3_flux_mean',\n",
      "       'interval_5_flux_mean_minus_interval_1_flux_mean',\n",
      "       'interval_5_flux_mean_minus_interval_2_flux_mean',\n",
      "       'interval_5_flux_mean_minus_interval_3_flux_mean',\n",
      "       'interval_5_flux_mean_minus_interval_4_flux_mean',\n",
      "       'interval_6_flux_mean_minus_interval_1_flux_mean',\n",
      "       'interval_6_flux_mean_minus_interval_2_flux_mean',\n",
      "       'interval_6_flux_mean_minus_interval_3_flux_mean',\n",
      "       'interval_6_flux_mean_minus_interval_4_flux_mean',\n",
      "       'interval_6_flux_mean_minus_interval_5_flux_mean'],\n",
      "      dtype='object', length=190)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.169033\ttraining's wloss: 0.301752\tvalid_1's multi_logloss: 0.202966\tvalid_1's wloss: 0.348914\n",
      "[200]\ttraining's multi_logloss: 0.0541496\ttraining's wloss: 0.122061\tvalid_1's multi_logloss: 0.100286\tvalid_1's wloss: 0.202674\n",
      "[300]\ttraining's multi_logloss: 0.0307363\ttraining's wloss: 0.0756457\tvalid_1's multi_logloss: 0.0827752\tvalid_1's wloss: 0.173819\n",
      "[400]\ttraining's multi_logloss: 0.0229738\ttraining's wloss: 0.0585223\tvalid_1's multi_logloss: 0.0762208\tvalid_1's wloss: 0.163894\n",
      "[500]\ttraining's multi_logloss: 0.0194245\ttraining's wloss: 0.050665\tvalid_1's multi_logloss: 0.0734935\tvalid_1's wloss: 0.159746\n",
      "[600]\ttraining's multi_logloss: 0.017266\ttraining's wloss: 0.0460914\tvalid_1's multi_logloss: 0.0718967\tvalid_1's wloss: 0.157539\n",
      "[700]\ttraining's multi_logloss: 0.0160478\ttraining's wloss: 0.043496\tvalid_1's multi_logloss: 0.0707719\tvalid_1's wloss: 0.155067\n",
      "[800]\ttraining's multi_logloss: 0.0151384\ttraining's wloss: 0.0414488\tvalid_1's multi_logloss: 0.0701767\tvalid_1's wloss: 0.15324\n",
      "[900]\ttraining's multi_logloss: 0.0145567\ttraining's wloss: 0.0400198\tvalid_1's multi_logloss: 0.0700064\tvalid_1's wloss: 0.151881\n",
      "[1000]\ttraining's multi_logloss: 0.0140529\ttraining's wloss: 0.0387386\tvalid_1's multi_logloss: 0.0697543\tvalid_1's wloss: 0.150071\n",
      "Early stopping, best iteration is:\n",
      "[1004]\ttraining's multi_logloss: 0.0140342\ttraining's wloss: 0.0386955\tvalid_1's multi_logloss: 0.0697193\tvalid_1's wloss: 0.14995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:05:59,989:main:0.1499498723033605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.163047\ttraining's wloss: 0.276593\tvalid_1's multi_logloss: 0.236771\tvalid_1's wloss: 0.595959\n",
      "[200]\ttraining's multi_logloss: 0.0494347\ttraining's wloss: 0.10914\tvalid_1's multi_logloss: 0.140613\tvalid_1's wloss: 0.501168\n",
      "[300]\ttraining's multi_logloss: 0.0284263\ttraining's wloss: 0.069684\tvalid_1's multi_logloss: 0.127117\tvalid_1's wloss: 0.470301\n",
      "[400]\ttraining's multi_logloss: 0.021736\ttraining's wloss: 0.055813\tvalid_1's multi_logloss: 0.12395\tvalid_1's wloss: 0.456676\n",
      "[500]\ttraining's multi_logloss: 0.0184705\ttraining's wloss: 0.0485095\tvalid_1's multi_logloss: 0.122793\tvalid_1's wloss: 0.45191\n",
      "[600]\ttraining's multi_logloss: 0.0168229\ttraining's wloss: 0.0445997\tvalid_1's multi_logloss: 0.122189\tvalid_1's wloss: 0.449461\n",
      "[700]\ttraining's multi_logloss: 0.0156467\ttraining's wloss: 0.0418909\tvalid_1's multi_logloss: 0.121268\tvalid_1's wloss: 0.445549\n",
      "[800]\ttraining's multi_logloss: 0.0147841\ttraining's wloss: 0.0398163\tvalid_1's multi_logloss: 0.120329\tvalid_1's wloss: 0.442357\n",
      "Early stopping, best iteration is:\n",
      "[797]\ttraining's multi_logloss: 0.0148031\ttraining's wloss: 0.0398779\tvalid_1's multi_logloss: 0.120268\tvalid_1's wloss: 0.442035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:06:06,287:main:0.44203528244246587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.16921\ttraining's wloss: 0.298611\tvalid_1's multi_logloss: 0.225788\tvalid_1's wloss: 0.394495\n",
      "[200]\ttraining's multi_logloss: 0.0539567\ttraining's wloss: 0.119552\tvalid_1's multi_logloss: 0.118112\tvalid_1's wloss: 0.257296\n",
      "[300]\ttraining's multi_logloss: 0.0304425\ttraining's wloss: 0.0720905\tvalid_1's multi_logloss: 0.0964898\tvalid_1's wloss: 0.224019\n",
      "[400]\ttraining's multi_logloss: 0.022831\ttraining's wloss: 0.0560229\tvalid_1's multi_logloss: 0.0884485\tvalid_1's wloss: 0.209887\n",
      "[500]\ttraining's multi_logloss: 0.0192832\ttraining's wloss: 0.0488656\tvalid_1's multi_logloss: 0.0850786\tvalid_1's wloss: 0.207034\n",
      "[600]\ttraining's multi_logloss: 0.017261\ttraining's wloss: 0.0448409\tvalid_1's multi_logloss: 0.0832601\tvalid_1's wloss: 0.205641\n",
      "[700]\ttraining's multi_logloss: 0.0160516\ttraining's wloss: 0.0421513\tvalid_1's multi_logloss: 0.0819943\tvalid_1's wloss: 0.203856\n",
      "[800]\ttraining's multi_logloss: 0.0152536\ttraining's wloss: 0.0403576\tvalid_1's multi_logloss: 0.080812\tvalid_1's wloss: 0.202507\n",
      "[900]\ttraining's multi_logloss: 0.0146531\ttraining's wloss: 0.0388628\tvalid_1's multi_logloss: 0.0801861\tvalid_1's wloss: 0.201497\n",
      "[1000]\ttraining's multi_logloss: 0.0141228\ttraining's wloss: 0.0375132\tvalid_1's multi_logloss: 0.0796495\tvalid_1's wloss: 0.200011\n",
      "[1100]\ttraining's multi_logloss: 0.0136588\ttraining's wloss: 0.0363696\tvalid_1's multi_logloss: 0.0790877\tvalid_1's wloss: 0.19863\n",
      "[1200]\ttraining's multi_logloss: 0.0132496\ttraining's wloss: 0.0354051\tvalid_1's multi_logloss: 0.0788138\tvalid_1's wloss: 0.198072\n",
      "[1300]\ttraining's multi_logloss: 0.0128941\ttraining's wloss: 0.034699\tvalid_1's multi_logloss: 0.078577\tvalid_1's wloss: 0.197456\n",
      "Early stopping, best iteration is:\n",
      "[1304]\ttraining's multi_logloss: 0.0128809\ttraining's wloss: 0.0346576\tvalid_1's multi_logloss: 0.0785296\tvalid_1's wloss: 0.197254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:06:16,647:main:0.1972541285358784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.172261\ttraining's wloss: 0.304318\tvalid_1's multi_logloss: 0.20288\tvalid_1's wloss: 0.417031\n",
      "[200]\ttraining's multi_logloss: 0.0548965\ttraining's wloss: 0.117153\tvalid_1's multi_logloss: 0.0921482\tvalid_1's wloss: 0.273234\n",
      "[300]\ttraining's multi_logloss: 0.0318718\ttraining's wloss: 0.0707637\tvalid_1's multi_logloss: 0.0716456\tvalid_1's wloss: 0.243382\n",
      "[400]\ttraining's multi_logloss: 0.0236814\ttraining's wloss: 0.0544754\tvalid_1's multi_logloss: 0.0648693\tvalid_1's wloss: 0.233342\n",
      "[500]\ttraining's multi_logloss: 0.0199544\ttraining's wloss: 0.0474491\tvalid_1's multi_logloss: 0.0613999\tvalid_1's wloss: 0.229562\n",
      "[600]\ttraining's multi_logloss: 0.0178056\ttraining's wloss: 0.0436429\tvalid_1's multi_logloss: 0.0594178\tvalid_1's wloss: 0.22784\n",
      "[700]\ttraining's multi_logloss: 0.0165051\ttraining's wloss: 0.0408568\tvalid_1's multi_logloss: 0.0583852\tvalid_1's wloss: 0.226314\n",
      "Early stopping, best iteration is:\n",
      "[710]\ttraining's multi_logloss: 0.0163973\ttraining's wloss: 0.0405942\tvalid_1's multi_logloss: 0.0582931\tvalid_1's wloss: 0.22601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:06:23,059:main:0.22600977501542485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.167726\ttraining's wloss: 0.293084\tvalid_1's multi_logloss: 0.228952\tvalid_1's wloss: 0.435358\n",
      "[200]\ttraining's multi_logloss: 0.0515094\ttraining's wloss: 0.114783\tvalid_1's multi_logloss: 0.126558\tvalid_1's wloss: 0.316019\n",
      "[300]\ttraining's multi_logloss: 0.0288055\ttraining's wloss: 0.071199\tvalid_1's multi_logloss: 0.110774\tvalid_1's wloss: 0.291484\n",
      "[400]\ttraining's multi_logloss: 0.0216337\ttraining's wloss: 0.0559576\tvalid_1's multi_logloss: 0.105288\tvalid_1's wloss: 0.287763\n",
      "[500]\ttraining's multi_logloss: 0.0185206\ttraining's wloss: 0.0490552\tvalid_1's multi_logloss: 0.103088\tvalid_1's wloss: 0.284832\n",
      "[600]\ttraining's multi_logloss: 0.0169532\ttraining's wloss: 0.0455655\tvalid_1's multi_logloss: 0.101445\tvalid_1's wloss: 0.282328\n",
      "[700]\ttraining's multi_logloss: 0.0157822\ttraining's wloss: 0.0428876\tvalid_1's multi_logloss: 0.100523\tvalid_1's wloss: 0.281044\n",
      "[800]\ttraining's multi_logloss: 0.0148047\ttraining's wloss: 0.040672\tvalid_1's multi_logloss: 0.100111\tvalid_1's wloss: 0.280855\n",
      "Early stopping, best iteration is:\n",
      "[802]\ttraining's multi_logloss: 0.014787\ttraining's wloss: 0.0406114\tvalid_1's multi_logloss: 0.10007\tvalid_1's wloss: 0.280623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:06:30,098:main:0.28062322449438176\n",
      "[INFO]2018-10-30 11:06:30,108:main:MULTI WEIGHTED LOG LOSS : 0.25886 \n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI WEIGHTED LOG LOSS : 0.25886 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:06:36,776:main:Index(['flux_min', 'flux_max', 'flux_mean', 'flux_median', 'flux_std',\n",
      "       'flux_skew', 'flux_err_min', 'flux_err_max', 'flux_err_mean',\n",
      "       'flux_err_median',\n",
      "       ...\n",
      "       'interval_3_flux_max', 'interval_4_flux_mean', 'interval_4_flux_min',\n",
      "       'interval_4_flux_max', 'interval_5_flux_mean', 'interval_5_flux_min',\n",
      "       'interval_5_flux_max', 'interval_6_flux_mean', 'interval_6_flux_min',\n",
      "       'interval_6_flux_max'],\n",
      "      dtype='object', length=134)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.917817\ttraining's wloss: 1.14406\tvalid_1's multi_logloss: 1.033\tvalid_1's wloss: 1.41066\n",
      "[200]\ttraining's multi_logloss: 0.722588\ttraining's wloss: 0.910475\tvalid_1's multi_logloss: 0.904551\tvalid_1's wloss: 1.31387\n",
      "[300]\ttraining's multi_logloss: 0.624858\ttraining's wloss: 0.766334\tvalid_1's multi_logloss: 0.866057\tvalid_1's wloss: 1.26616\n",
      "[400]\ttraining's multi_logloss: 0.551629\ttraining's wloss: 0.65004\tvalid_1's multi_logloss: 0.847394\tvalid_1's wloss: 1.24643\n",
      "[500]\ttraining's multi_logloss: 0.492913\ttraining's wloss: 0.561163\tvalid_1's multi_logloss: 0.836207\tvalid_1's wloss: 1.23775\n",
      "[600]\ttraining's multi_logloss: 0.443096\ttraining's wloss: 0.489882\tvalid_1's multi_logloss: 0.830026\tvalid_1's wloss: 1.23795\n",
      "Early stopping, best iteration is:\n",
      "[550]\ttraining's multi_logloss: 0.466955\ttraining's wloss: 0.523276\tvalid_1's multi_logloss: 0.832682\tvalid_1's wloss: 1.23586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:06:49,377:main:1.2358575637252864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.918311\ttraining's wloss: 1.15155\tvalid_1's multi_logloss: 1.01533\tvalid_1's wloss: 1.31413\n",
      "[200]\ttraining's multi_logloss: 0.718478\ttraining's wloss: 0.902559\tvalid_1's multi_logloss: 0.886493\tvalid_1's wloss: 1.20885\n",
      "[300]\ttraining's multi_logloss: 0.61689\ttraining's wloss: 0.743767\tvalid_1's multi_logloss: 0.849332\tvalid_1's wloss: 1.18164\n",
      "Early stopping, best iteration is:\n",
      "[349]\ttraining's multi_logloss: 0.578592\ttraining's wloss: 0.683561\tvalid_1's multi_logloss: 0.839556\tvalid_1's wloss: 1.17803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:06:58,372:main:1.178028945457796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.909633\ttraining's wloss: 1.14683\tvalid_1's multi_logloss: 1.04151\tvalid_1's wloss: 1.33722\n",
      "[200]\ttraining's multi_logloss: 0.707384\ttraining's wloss: 0.899418\tvalid_1's multi_logloss: 0.92188\tvalid_1's wloss: 1.23414\n",
      "[300]\ttraining's multi_logloss: 0.605347\ttraining's wloss: 0.737562\tvalid_1's multi_logloss: 0.890519\tvalid_1's wloss: 1.19966\n",
      "[400]\ttraining's multi_logloss: 0.532527\ttraining's wloss: 0.620107\tvalid_1's multi_logloss: 0.877816\tvalid_1's wloss: 1.19406\n",
      "Early stopping, best iteration is:\n",
      "[362]\ttraining's multi_logloss: 0.557966\ttraining's wloss: 0.66049\tvalid_1's multi_logloss: 0.881754\tvalid_1's wloss: 1.19321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:07:07,710:main:1.193212058918227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.917181\ttraining's wloss: 1.14569\tvalid_1's multi_logloss: 1.01141\tvalid_1's wloss: 1.34203\n",
      "[200]\ttraining's multi_logloss: 0.720441\ttraining's wloss: 0.901372\tvalid_1's multi_logloss: 0.88418\tvalid_1's wloss: 1.23935\n",
      "[300]\ttraining's multi_logloss: 0.618191\ttraining's wloss: 0.744526\tvalid_1's multi_logloss: 0.841344\tvalid_1's wloss: 1.19938\n",
      "[400]\ttraining's multi_logloss: 0.544169\ttraining's wloss: 0.632313\tvalid_1's multi_logloss: 0.82082\tvalid_1's wloss: 1.1852\n",
      "[500]\ttraining's multi_logloss: 0.48545\ttraining's wloss: 0.54609\tvalid_1's multi_logloss: 0.811667\tvalid_1's wloss: 1.17922\n",
      "Early stopping, best iteration is:\n",
      "[479]\ttraining's multi_logloss: 0.49665\ttraining's wloss: 0.562252\tvalid_1's multi_logloss: 0.812823\tvalid_1's wloss: 1.17827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:07:19,463:main:1.1782744783079875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.920307\ttraining's wloss: 1.15323\tvalid_1's multi_logloss: 1.01103\tvalid_1's wloss: 1.3544\n",
      "[200]\ttraining's multi_logloss: 0.72458\ttraining's wloss: 0.914955\tvalid_1's multi_logloss: 0.882937\tvalid_1's wloss: 1.24197\n",
      "[300]\ttraining's multi_logloss: 0.623648\ttraining's wloss: 0.754206\tvalid_1's multi_logloss: 0.842624\tvalid_1's wloss: 1.19038\n",
      "[400]\ttraining's multi_logloss: 0.549989\ttraining's wloss: 0.636779\tvalid_1's multi_logloss: 0.824321\tvalid_1's wloss: 1.1719\n",
      "[500]\ttraining's multi_logloss: 0.49327\ttraining's wloss: 0.551999\tvalid_1's multi_logloss: 0.816096\tvalid_1's wloss: 1.16691\n",
      "Early stopping, best iteration is:\n",
      "[462]\ttraining's multi_logloss: 0.513579\ttraining's wloss: 0.582307\tvalid_1's multi_logloss: 0.817941\tvalid_1's wloss: 1.16668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2018-10-30 11:07:30,747:main:1.1666816449932116\n",
      "[INFO]2018-10-30 11:07:30,757:main:MULTI WEIGHTED LOG LOSS : 1.19084 \n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI WEIGHTED LOG LOSS : 1.19084 \n",
      "Feature hostgal_specz is removed\n",
      "Chunk size 499895\n",
      "Dimension of merge data for MJD relevant data and META data  (499895, 16)\n",
      "Number of objects in galaxy : 13\n",
      "Number of objects out of galaxy : 1504\n",
      "Just to check, sum of objects : 1517\n",
      "Total number should be 1517\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (4352, 16) (495543, 16)\n",
      "Dimension of aggregated data on flux features (1517, 18)\n",
      "Dimension of merge data for Object relevant data and META data (1517, 29)\n",
      "Number of objects in galaxy : 13\n",
      "Number of objects out of galaxy : 1504\n",
      "Just to check, sum of objects : 1517\n",
      "Total number should be 1517\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (13, 29) (1504, 29)\n",
      "Features extraction begins...\n",
      "In terms of that in the Galaxy...\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n",
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n",
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n",
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n",
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Min and Max MJD time : 59582.3282, 60674.0798\n",
      "Interval #1, record quantity: 214\n",
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_std', 'interval_1_flux_min', 'interval_1_flux_max', 'interval_1_flux_skew']\n",
      "Interval #2, record quantity: 1300\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_std', 'interval_2_flux_min', 'interval_2_flux_max', 'interval_2_flux_skew']\n",
      "Interval #3, record quantity: 216\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_std', 'interval_3_flux_min', 'interval_3_flux_max', 'interval_3_flux_skew']\n",
      "Interval #4, record quantity: 1110\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_std', 'interval_4_flux_min', 'interval_4_flux_max', 'interval_4_flux_skew']\n",
      "Interval #5, record quantity: 200\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min', 'interval_5_flux_max', 'interval_5_flux_skew']\n",
      "Interval #6, record quantity: 1308\n",
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max', 'interval_6_flux_skew']\n",
      "Feature added:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:312: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:321: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:330: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:346: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:362: FutureWarning: 'object_id' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:528: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " interval_2_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_3_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_3_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_4_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_4_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_5_flux_max\n",
      "Feature added: interval_2_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_3_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_3_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_4_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_4_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_5_flux_min\n",
      "Feature added: interval_2_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_3_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_3_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_4_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_4_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_5_flux_mean\n",
      "Dimension of data after adding features relevant to time intervals (13, 191)\n",
      "In terms of that out of the Galaxy...\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n",
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n",
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n",
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n",
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Min and Max MJD time : 59582.3282, 60674.0798\n",
      "Interval #1, record quantity: 26064\n",
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_min', 'interval_1_flux_max']\n",
      "Interval #2, record quantity: 147290\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_min', 'interval_2_flux_max']\n",
      "Interval #3, record quantity: 29691\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_min', 'interval_3_flux_max']\n",
      "Interval #4, record quantity: 122002\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_min', 'interval_4_flux_max']\n",
      "Interval #5, record quantity: 27915\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_min', 'interval_5_flux_max']\n",
      "Interval #6, record quantity: 142278\n",
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_min', 'interval_6_flux_max']\n",
      "Dimension of data after adding features relevant to time intervals (1504, 135)\n",
      "[  168  1063  2270  2790  4025  4109  8442  8933 10946 11786 13619 14235\n",
      " 14982]\n",
      "[   13    14    17 ... 15578 15585 15596]\n",
      "            15        42        52        62        64        67        88  \\\n",
      "0     0.002518  0.421071  0.043132  0.031720  0.000644  0.005190  0.000548   \n",
      "1     0.005870  0.111538  0.013487  0.048080  0.004588  0.012873  0.002609   \n",
      "2     0.005199  0.063914  0.008251  0.018703  0.002576  0.014331  0.012107   \n",
      "3     0.003607  0.180767  0.011389  0.035232  0.003031  0.082363  0.001506   \n",
      "4     0.002757  0.052364  0.058403  0.009010  0.000339  0.007134  0.000275   \n",
      "5     0.001487  0.088227  0.023948  0.020561  0.000804  0.022018  0.000959   \n",
      "6     0.003575  0.073849  0.007458  0.045654  0.002253  0.029136  0.001142   \n",
      "7     0.013054  0.107533  0.022200  0.220626  0.015942  0.043329  0.005601   \n",
      "8     0.001938  0.058700  0.013187  0.023175  0.000857  0.043231  0.000471   \n",
      "9     0.003403  0.176479  0.022827  0.046739  0.001437  0.070342  0.004236   \n",
      "10    0.001465  0.877066  0.027687  0.071182  0.000208  0.001372  0.000240   \n",
      "11    0.277512  0.634739  0.005354  0.007480  0.000531  0.001409  0.001098   \n",
      "12    0.009106  0.118506  0.013707  0.024050  0.002275  0.016072  0.001488   \n",
      "13    0.002301  0.080839  0.018219  0.028979  0.000585  0.056569  0.000611   \n",
      "14    0.025148  0.242217  0.012270  0.035788  0.020093  0.017402  0.011234   \n",
      "15    0.009152  0.206219  0.014481  0.135116  0.017691  0.075333  0.042575   \n",
      "16    0.001541  0.048506  0.067589  0.009495  0.000379  0.009565  0.000263   \n",
      "17    0.007901  0.197370  0.007717  0.144294  0.012246  0.037013  0.004072   \n",
      "18    0.023379  0.259957  0.012903  0.016860  0.001241  0.028121  0.000674   \n",
      "19    0.003529  0.124113  0.043311  0.022434  0.000657  0.007624  0.000482   \n",
      "20    0.002924  0.047469  0.005544  0.029558  0.005860  0.027174  0.000956   \n",
      "21    0.005337  0.114243  0.015370  0.015000  0.001003  0.008937  0.006024   \n",
      "22    0.020048  0.829287  0.030847  0.011151  0.000617  0.002210  0.000925   \n",
      "23    0.005063  0.086038  0.008052  0.024125  0.004108  0.042744  0.003261   \n",
      "24    0.002109  0.089580  0.022266  0.025009  0.000576  0.018128  0.000441   \n",
      "25    0.004700  0.156271  0.062356  0.103030  0.000489  0.005317  0.000571   \n",
      "26    0.004069  0.193718  0.179015  0.010306  0.000388  0.001971  0.000374   \n",
      "27    0.004142  0.089449  0.009456  0.009699  0.000951  0.025365  0.002098   \n",
      "28    0.002655  0.087320  0.004177  0.039788  0.001281  0.025153  0.000977   \n",
      "29    0.001952  0.073570  0.067016  0.007955  0.000401  0.010498  0.000279   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1474  0.003998  0.086917  0.006585  0.064846  0.006455  0.012486  0.000515   \n",
      "1475  0.005016  0.934743  0.017927  0.011767  0.000331  0.001190  0.001290   \n",
      "1476  0.007114  0.065118  0.008197  0.022421  0.002495  0.025868  0.001563   \n",
      "1477  0.004442  0.178420  0.004409  0.101723  0.010327  0.033033  0.004440   \n",
      "1478  0.001291  0.087298  0.016388  0.013432  0.000386  0.019260  0.000314   \n",
      "1479  0.002322  0.061694  0.019253  0.012185  0.001047  0.023847  0.000724   \n",
      "1480  0.004749  0.128947  0.034530  0.053279  0.000561  0.043848  0.000393   \n",
      "1481  0.013181  0.328488  0.007616  0.245062  0.016670  0.096244  0.009237   \n",
      "1482  0.011244  0.166599  0.016954  0.038815  0.030725  0.057542  0.012974   \n",
      "1483  0.011766  0.246671  0.021741  0.008081  0.000640  0.007356  0.001034   \n",
      "1484  0.018921  0.272768  0.015938  0.009689  0.001072  0.008060  0.000615   \n",
      "1485  0.001866  0.116383  0.018355  0.014283  0.000314  0.027888  0.000320   \n",
      "1486  0.013857  0.133283  0.010136  0.065008  0.008856  0.094536  0.006337   \n",
      "1487  0.002971  0.047806  0.010423  0.013894  0.002932  0.010508  0.001871   \n",
      "1488  0.007159  0.649508  0.023485  0.005587  0.000551  0.005114  0.002059   \n",
      "1489  0.003055  0.107087  0.061579  0.019859  0.001170  0.015145  0.002337   \n",
      "1490  0.005674  0.327960  0.203484  0.030283  0.001119  0.026418  0.015209   \n",
      "1491  0.005034  0.180400  0.036513  0.058313  0.003574  0.120116  0.001086   \n",
      "1492  0.001716  0.034316  0.016117  0.007308  0.000229  0.001729  0.000243   \n",
      "1493  0.011709  0.113699  0.011481  0.137834  0.012468  0.025418  0.005112   \n",
      "1494  0.022225  0.123368  0.010088  0.018064  0.006804  0.020356  0.066449   \n",
      "1495  0.005283  0.060844  0.037971  0.011286  0.003711  0.053188  0.003063   \n",
      "1496  0.004844  0.077549  0.115014  0.011794  0.000532  0.013260  0.000616   \n",
      "1497  0.003098  0.110987  0.047560  0.021405  0.000364  0.006597  0.000909   \n",
      "1498  0.008156  0.152483  0.018347  0.013194  0.001654  0.022339  0.004427   \n",
      "1499  0.002992  0.100966  0.026023  0.054763  0.001095  0.043869  0.000674   \n",
      "1500  0.001956  0.076276  0.016075  0.027936  0.000600  0.029256  0.000443   \n",
      "1501  0.002086  0.097716  0.017214  0.012604  0.000834  0.015955  0.000397   \n",
      "1502  0.007359  0.131267  0.008427  0.014456  0.000877  0.005528  0.003084   \n",
      "1503  0.010231  0.156713  0.012342  0.048458  0.012090  0.025155  0.005976   \n",
      "\n",
      "            90        95  \n",
      "0     0.493183  0.001995  \n",
      "1     0.794886  0.006068  \n",
      "2     0.867410  0.007508  \n",
      "3     0.675814  0.006292  \n",
      "4     0.868779  0.000940  \n",
      "5     0.831261  0.010736  \n",
      "6     0.828868  0.008065  \n",
      "7     0.466338  0.105376  \n",
      "8     0.841007  0.017434  \n",
      "9     0.405020  0.269516  \n",
      "10    0.019796  0.000984  \n",
      "11    0.068562  0.003315  \n",
      "12    0.802982  0.011814  \n",
      "13    0.806170  0.005727  \n",
      "14    0.603131  0.032717  \n",
      "15    0.357588  0.141846  \n",
      "16    0.862172  0.000489  \n",
      "17    0.261154  0.328233  \n",
      "18    0.656094  0.000771  \n",
      "19    0.795197  0.002653  \n",
      "20    0.872295  0.008220  \n",
      "21    0.832990  0.001096  \n",
      "22    0.102107  0.002808  \n",
      "23    0.811496  0.015113  \n",
      "24    0.822519  0.019372  \n",
      "25    0.665307  0.001959  \n",
      "26    0.608639  0.001520  \n",
      "27    0.856264  0.002575  \n",
      "28    0.835034  0.003616  \n",
      "29    0.837787  0.000541  \n",
      "...        ...       ...  \n",
      "1474  0.814584  0.003614  \n",
      "1475  0.024143  0.003593  \n",
      "1476  0.859101  0.008123  \n",
      "1477  0.632691  0.030513  \n",
      "1478  0.852815  0.008815  \n",
      "1479  0.871984  0.006944  \n",
      "1480  0.733125  0.000568  \n",
      "1481  0.252169  0.031334  \n",
      "1482  0.527085  0.138061  \n",
      "1483  0.698567  0.004143  \n",
      "1484  0.671309  0.001627  \n",
      "1485  0.819479  0.001112  \n",
      "1486  0.472310  0.195676  \n",
      "1487  0.904538  0.005057  \n",
      "1488  0.299607  0.006930  \n",
      "1489  0.788447  0.001322  \n",
      "1490  0.387476  0.002377  \n",
      "1491  0.361168  0.233795  \n",
      "1492  0.937824  0.000519  \n",
      "1493  0.536529  0.145752  \n",
      "1494  0.730871  0.001777  \n",
      "1495  0.815864  0.008790  \n",
      "1496  0.775674  0.000717  \n",
      "1497  0.793220  0.015860  \n",
      "1498  0.759938  0.019460  \n",
      "1499  0.756025  0.013594  \n",
      "1500  0.825368  0.022089  \n",
      "1501  0.851669  0.001526  \n",
      "1502  0.768067  0.060935  \n",
      "1503  0.680969  0.048065  \n",
      "\n",
      "[1504 rows x 9 columns]\n",
      "(1517, 14)\n",
      "0.13999999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size 499845\n",
      "Dimension of merge data for MJD relevant data and META data  (499845, 16)\n",
      "Number of objects in galaxy : 4\n",
      "Number of objects out of galaxy : 1514\n",
      "Just to check, sum of objects : 1518\n",
      "Total number should be 1518\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (1362, 16) (498483, 16)\n",
      "Dimension of aggregated data on flux features (1518, 18)\n",
      "Dimension of merge data for Object relevant data and META data (1518, 29)\n",
      "Number of objects in galaxy : 4\n",
      "Number of objects out of galaxy : 1514\n",
      "Just to check, sum of objects : 1518\n",
      "Total number should be 1518\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (4, 29) (1514, 29)\n",
      "Features extraction begins...\n",
      "In terms of that in the Galaxy...\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n",
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n",
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n",
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n",
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Min and Max MJD time : 59750.4229, 60674.0798\n",
      "Interval #1, record quantity: 366\n",
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_std', 'interval_1_flux_min', 'interval_1_flux_max', 'interval_1_flux_skew']\n",
      "Interval #2, record quantity: 125\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_std', 'interval_2_flux_min', 'interval_2_flux_max', 'interval_2_flux_skew']\n",
      "Interval #3, record quantity: 175\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_std', 'interval_3_flux_min', 'interval_3_flux_max', 'interval_3_flux_skew']\n",
      "Interval #4, record quantity: 232\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_std', 'interval_4_flux_min', 'interval_4_flux_max', 'interval_4_flux_skew']\n",
      "Interval #5, record quantity: 19\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_std', 'interval_5_flux_min', 'interval_5_flux_max', 'interval_5_flux_skew']\n",
      "Interval #6, record quantity: 444\n",
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_std', 'interval_6_flux_min', 'interval_6_flux_max', 'interval_6_flux_skew']\n",
      "Feature added: interval_2_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_3_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_3_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_4_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_5_flux_max_minus_interval_4_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_1_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_2_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_3_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_4_flux_max\n",
      "Feature added: interval_6_flux_max_minus_interval_5_flux_max\n",
      "Feature added: interval_2_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_3_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_3_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_4_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_5_flux_min_minus_interval_4_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_1_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_2_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_3_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_4_flux_min\n",
      "Feature added: interval_6_flux_min_minus_interval_5_flux_min\n",
      "Feature added: interval_2_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_3_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_3_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_4_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_5_flux_mean_minus_interval_4_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_1_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_2_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_3_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_4_flux_mean\n",
      "Feature added: interval_6_flux_mean_minus_interval_5_flux_mean\n",
      "Dimension of data after adding features relevant to time intervals (4, 191)\n",
      "In terms of that out of the Galaxy...\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n",
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n",
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n",
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Min and Max MJD time : 59582.3282, 60674.0798\n",
      "Interval #1, record quantity: 26223\n",
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_min', 'interval_1_flux_max']\n",
      "Interval #2, record quantity: 148237\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_min', 'interval_2_flux_max']\n",
      "Interval #3, record quantity: 29822\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_min', 'interval_3_flux_max']\n",
      "Interval #4, record quantity: 122633\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_min', 'interval_4_flux_max']\n",
      "Interval #5, record quantity: 28040\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_min', 'interval_5_flux_max']\n",
      "Interval #6, record quantity: 143204\n",
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_min', 'interval_6_flux_max']\n",
      "Dimension of data after adding features relevant to time intervals (1514, 135)\n",
      "[25882 26083 28505 29710]\n",
      "[15634 15635 15663 ... 32269 32284 32293]\n",
      "            15        42        52        62        64        67        88  \\\n",
      "0     0.010890  0.154714  0.008427  0.106431  0.006238  0.016085  0.007818   \n",
      "1     0.003448  0.474337  0.141286  0.051211  0.000707  0.006046  0.001083   \n",
      "2     0.003819  0.220088  0.077257  0.020306  0.001003  0.016584  0.001014   \n",
      "3     0.001813  0.084800  0.015954  0.014562  0.000392  0.011329  0.000309   \n",
      "4     0.002107  0.080845  0.032536  0.010442  0.000278  0.002324  0.000334   \n",
      "5     0.002705  0.052159  0.017756  0.005278  0.000548  0.019415  0.001018   \n",
      "6     0.007312  0.157258  0.012462  0.035356  0.013088  0.075984  0.047322   \n",
      "7     0.015376  0.178914  0.019386  0.051477  0.005616  0.017399  0.006418   \n",
      "8     0.001622  0.082009  0.039124  0.014583  0.000334  0.009491  0.000446   \n",
      "9     0.006756  0.130342  0.018734  0.035435  0.002170  0.027735  0.001568   \n",
      "10    0.004746  0.329939  0.013888  0.045341  0.004041  0.064328  0.001594   \n",
      "11    0.001495  0.054962  0.041825  0.008612  0.000225  0.003441  0.000276   \n",
      "12    0.004079  0.177710  0.008835  0.024215  0.002986  0.059182  0.006577   \n",
      "13    0.004122  0.089717  0.014456  0.016760  0.000975  0.037572  0.001468   \n",
      "14    0.028511  0.222436  0.017169  0.030918  0.003007  0.022158  0.004045   \n",
      "15    0.004707  0.152733  0.010722  0.012792  0.001396  0.014011  0.001011   \n",
      "16    0.008770  0.097516  0.006115  0.033523  0.003162  0.027725  0.004137   \n",
      "17    0.008198  0.260799  0.012176  0.116581  0.008554  0.161254  0.003560   \n",
      "18    0.019796  0.194559  0.013394  0.037839  0.001673  0.014704  0.003525   \n",
      "19    0.003474  0.157064  0.020952  0.037034  0.000599  0.034585  0.000510   \n",
      "20    0.007150  0.572372  0.064965  0.032173  0.000828  0.006776  0.000855   \n",
      "21    0.011564  0.106221  0.011836  0.106745  0.017471  0.015726  0.057018   \n",
      "22    0.003103  0.168291  0.007435  0.086314  0.001045  0.044167  0.000839   \n",
      "23    0.004300  0.449841  0.006903  0.201464  0.020546  0.044358  0.004490   \n",
      "24    0.002039  0.109485  0.047972  0.025803  0.000538  0.021761  0.000471   \n",
      "25    0.001988  0.191717  0.053183  0.561581  0.000386  0.032601  0.000411   \n",
      "26    0.026009  0.107916  0.016218  0.038295  0.013715  0.029768  0.005271   \n",
      "27    0.006167  0.080185  0.011461  0.065187  0.006603  0.032009  0.013205   \n",
      "28    0.001983  0.182667  0.022856  0.054225  0.000519  0.013593  0.001171   \n",
      "29    0.006900  0.149882  0.006477  0.232694  0.013262  0.064928  0.009130   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1484  0.014202  0.574989  0.009081  0.052278  0.000918  0.012081  0.001683   \n",
      "1485  0.003539  0.169853  0.024220  0.042774  0.001353  0.072733  0.001072   \n",
      "1486  0.006226  0.266651  0.024248  0.010633  0.001084  0.008605  0.001328   \n",
      "1487  0.002734  0.549384  0.106219  0.246287  0.000612  0.008953  0.000731   \n",
      "1488  0.008503  0.107023  0.011988  0.163304  0.010411  0.054893  0.011671   \n",
      "1489  0.002509  0.120204  0.017433  0.048930  0.001447  0.023938  0.000568   \n",
      "1490  0.002250  0.111360  0.092083  0.029033  0.000415  0.003780  0.000516   \n",
      "1491  0.010499  0.172321  0.004970  0.096875  0.002247  0.021235  0.001133   \n",
      "1492  0.003142  0.129662  0.023878  0.032584  0.000564  0.031432  0.000645   \n",
      "1493  0.009536  0.143648  0.015905  0.016195  0.002142  0.016142  0.002637   \n",
      "1494  0.002923  0.090030  0.015584  0.020605  0.000902  0.036794  0.000586   \n",
      "1495  0.005232  0.166726  0.028579  0.043254  0.001126  0.055061  0.000803   \n",
      "1496  0.005323  0.090539  0.006136  0.064245  0.003525  0.117618  0.000702   \n",
      "1497  0.003534  0.280099  0.006928  0.044610  0.004311  0.036652  0.005632   \n",
      "1498  0.003908  0.225792  0.006845  0.090149  0.002165  0.074174  0.005264   \n",
      "1499  0.002451  0.097173  0.025085  0.237234  0.000757  0.104859  0.000549   \n",
      "1500  0.002464  0.015274  0.012693  0.003613  0.000175  0.002463  0.000167   \n",
      "1501  0.003553  0.098364  0.019814  0.014608  0.000534  0.028364  0.000475   \n",
      "1502  0.007199  0.146663  0.006646  0.079815  0.004262  0.034402  0.003692   \n",
      "1503  0.069004  0.423058  0.015799  0.017193  0.001434  0.007095  0.001964   \n",
      "1504  0.003335  0.084110  0.029028  0.016380  0.001011  0.015061  0.001090   \n",
      "1505  0.012230  0.198974  0.008170  0.029499  0.002516  0.009500  0.003996   \n",
      "1506  0.002899  0.181613  0.024447  0.066228  0.001524  0.059746  0.001127   \n",
      "1507  0.010347  0.075057  0.021176  0.042805  0.002616  0.016776  0.003021   \n",
      "1508  0.002675  0.071266  0.032532  0.010042  0.000767  0.028605  0.000839   \n",
      "1509  0.011505  0.896355  0.014397  0.013586  0.000462  0.002869  0.000472   \n",
      "1510  0.003959  0.061551  0.013291  0.008460  0.000941  0.023896  0.002041   \n",
      "1511  0.004314  0.109365  0.052032  0.029481  0.001638  0.046674  0.002162   \n",
      "1512  0.020808  0.202353  0.019303  0.262560  0.037015  0.017577  0.072008   \n",
      "1513  0.003264  0.112953  0.075181  0.010305  0.000385  0.002989  0.000444   \n",
      "\n",
      "            90        95  \n",
      "0     0.668743  0.020655  \n",
      "1     0.313010  0.008872  \n",
      "2     0.658125  0.001803  \n",
      "3     0.860114  0.010726  \n",
      "4     0.869930  0.001203  \n",
      "5     0.898332  0.002789  \n",
      "6     0.517954  0.133263  \n",
      "7     0.666753  0.038661  \n",
      "8     0.850431  0.001960  \n",
      "9     0.774695  0.002566  \n",
      "10    0.233255  0.302869  \n",
      "11    0.888148  0.001017  \n",
      "12    0.707171  0.009245  \n",
      "13    0.831371  0.003559  \n",
      "14    0.654897  0.016858  \n",
      "15    0.799441  0.003187  \n",
      "16    0.806205  0.012848  \n",
      "17    0.374562  0.054317  \n",
      "18    0.681538  0.032971  \n",
      "19    0.743604  0.002179  \n",
      "20    0.313305  0.001577  \n",
      "21    0.652126  0.021294  \n",
      "22    0.668517  0.020287  \n",
      "23    0.187580  0.080519  \n",
      "24    0.759237  0.032694  \n",
      "25    0.157541  0.000594  \n",
      "26    0.594908  0.167900  \n",
      "27    0.631747  0.153435  \n",
      "28    0.693090  0.029896  \n",
      "29    0.318725  0.198001  \n",
      "...        ...       ...  \n",
      "1484  0.329415  0.005352  \n",
      "1485  0.324963  0.359492  \n",
      "1486  0.671477  0.009747  \n",
      "1487  0.083414  0.001666  \n",
      "1488  0.476524  0.155683  \n",
      "1489  0.763734  0.021236  \n",
      "1490  0.739059  0.021505  \n",
      "1491  0.657693  0.033027  \n",
      "1492  0.771586  0.006506  \n",
      "1493  0.790404  0.003391  \n",
      "1494  0.829154  0.003422  \n",
      "1495  0.516560  0.182657  \n",
      "1496  0.711277  0.000635  \n",
      "1497  0.267111  0.351123  \n",
      "1498  0.583435  0.008268  \n",
      "1499  0.470072  0.061822  \n",
      "1500  0.962596  0.000554  \n",
      "1501  0.827447  0.006843  \n",
      "1502  0.705157  0.012164  \n",
      "1503  0.460613  0.003841  \n",
      "1504  0.848052  0.001932  \n",
      "1505  0.691855  0.043260  \n",
      "1506  0.435819  0.226597  \n",
      "1507  0.661570  0.166632  \n",
      "1508  0.849411  0.003863  \n",
      "1509  0.058496  0.001858  \n",
      "1510  0.883828  0.002034  \n",
      "1511  0.753416  0.000918  \n",
      "1512  0.169357  0.199018  \n",
      "1513  0.775069  0.019410  \n",
      "\n",
      "[1514 rows x 9 columns]\n",
      "(1518, 14)\n",
      "0.13999999999999999\n",
      "Chunk size 260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of merge data for MJD relevant data and META data  (260, 16)\n",
      "Number of objects in galaxy : 0\n",
      "Number of objects out of galaxy : 1\n",
      "Just to check, sum of objects : 1\n",
      "Total number should be 1\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (0, 16) (260, 16)\n",
      "Dimension of aggregated data on flux features (1, 18)\n",
      "Dimension of merge data for Object relevant data and META data (1, 29)\n",
      "Number of objects in galaxy : 0\n",
      "Number of objects out of galaxy : 1\n",
      "Just to check, sum of objects : 1\n",
      "Total number should be 1\n",
      "Dimension of merge data for that in galaxy and that out of galaxy  (0, 29) (1, 29)\n",
      "Features extraction begins...\n",
      "Object relevant data in the Galaxy has no data, nothing to predict.\n",
      "In terms of that out of the Galaxy...\n",
      "Adding feats for the flux mean per band...\n",
      "Feats added: ['band_0_flux_mean', 'band_1_flux_mean', 'band_2_flux_mean', 'band_3_flux_mean', 'band_4_flux_mean', 'band_5_flux_mean', 'band_1_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_0_flux_mean', 'band_2_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_0_flux_mean', 'band_3_flux_mean_minus_band_1_flux_mean', 'band_3_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_0_flux_mean', 'band_4_flux_mean_minus_band_1_flux_mean', 'band_4_flux_mean_minus_band_2_flux_mean', 'band_4_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_0_flux_mean', 'band_5_flux_mean_minus_band_1_flux_mean', 'band_5_flux_mean_minus_band_2_flux_mean', 'band_5_flux_mean_minus_band_3_flux_mean', 'band_5_flux_mean_minus_band_4_flux_mean']\n",
      "Adding feats for the flux std per band...\n",
      "Feats added: ['band_0_flux_std', 'band_1_flux_std', 'band_2_flux_std', 'band_3_flux_std', 'band_4_flux_std', 'band_5_flux_std']\n",
      "Adding feats for the flux skew per band...\n",
      "Feats added: ['band_0_flux_skew', 'band_1_flux_skew', 'band_2_flux_skew', 'band_3_flux_skew', 'band_4_flux_skew', 'band_5_flux_skew']\n",
      "Adding feats for the flux max per band...\n",
      "Feats added: ['band_0_flux_max', 'band_1_flux_max', 'band_2_flux_max', 'band_3_flux_max', 'band_4_flux_max', 'band_5_flux_max', 'band_1_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_0_flux_max', 'band_2_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_0_flux_max', 'band_3_flux_max_minus_band_1_flux_max', 'band_3_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_0_flux_max', 'band_4_flux_max_minus_band_1_flux_max', 'band_4_flux_max_minus_band_2_flux_max', 'band_4_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_0_flux_max', 'band_5_flux_max_minus_band_1_flux_max', 'band_5_flux_max_minus_band_2_flux_max', 'band_5_flux_max_minus_band_3_flux_max', 'band_5_flux_max_minus_band_4_flux_max']\n",
      "Adding feats for the flux min per band...\n",
      "Feats added: ['band_0_flux_min', 'band_1_flux_min', 'band_2_flux_min', 'band_3_flux_min', 'band_4_flux_min', 'band_5_flux_min', 'band_1_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_0_flux_min', 'band_2_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_0_flux_min', 'band_3_flux_min_minus_band_1_flux_min', 'band_3_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_0_flux_min', 'band_4_flux_min_minus_band_1_flux_min', 'band_4_flux_min_minus_band_2_flux_min', 'band_4_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_0_flux_min', 'band_5_flux_min_minus_band_1_flux_min', 'band_5_flux_min_minus_band_2_flux_min', 'band_5_flux_min_minus_band_3_flux_min', 'band_5_flux_min_minus_band_4_flux_min']\n",
      "Adding feats for the flux (max-min)/mean per band...\n",
      "Feature added: band_0_flux_diff2\n",
      "Feature added: band_1_flux_diff2\n",
      "Feature added: band_2_flux_diff2\n",
      "Feature added: band_3_flux_diff2\n",
      "Feature added: band_4_flux_diff2\n",
      "Feature added: band_5_flux_diff2\n",
      "Number of Intervals : 6\n",
      "Min and Max MJD time : 59798.3205, 60587.154\n",
      "Interval #1, record quantity: 106\n",
      "New features added:  ['object_id', 'interval_1_flux_mean', 'interval_1_flux_min', 'interval_1_flux_max']\n",
      "Interval #2, record quantity: 16\n",
      "New features added:  ['object_id', 'interval_2_flux_mean', 'interval_2_flux_min', 'interval_2_flux_max']\n",
      "Interval #3, record quantity: 20\n",
      "New features added:  ['object_id', 'interval_3_flux_mean', 'interval_3_flux_min', 'interval_3_flux_max']\n",
      "Interval #4, record quantity: 80\n",
      "New features added:  ['object_id', 'interval_4_flux_mean', 'interval_4_flux_min', 'interval_4_flux_max']\n",
      "Interval #5, record quantity: 0\n",
      "New features added:  ['object_id', 'interval_5_flux_mean', 'interval_5_flux_min', 'interval_5_flux_max']\n",
      "Interval #6, record quantity: 37\n",
      "New features added:  ['object_id', 'interval_6_flux_mean', 'interval_6_flux_min', 'interval_6_flux_max']\n",
      "Dimension of data after adding features relevant to time intervals (1, 135)\n",
      "[32300]\n",
      "        15        42        52        62        64        67        88  \\\n",
      "0  0.00178  0.071338  0.024013  0.028076  0.000453  0.030701  0.000461   \n",
      "\n",
      "         90        95  \n",
      "0  0.832906  0.010273  \n",
      "(1, 14)\n",
      "0.14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "\n",
    "\n",
    "def create_logger():\n",
    "    logger_ = logging.getLogger('main')\n",
    "    logger_.setLevel(logging.DEBUG)\n",
    "    fh = logging.FileHandler('simple_lightgbm.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    # add the handlers to the logger\n",
    "    logger_.addHandler(fh)\n",
    "    logger_.addHandler(ch)\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    return logging.getLogger('main')\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    \n",
    "    \n",
    "    if len(np.unique(y_true)) == 14:\n",
    "        classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "        class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "#     if len(np.unique(y_true)) > 14:\n",
    "#         classes.append(99)\n",
    "#         class_weight[99] = 2\n",
    "    \n",
    "    # Galaxy Case\n",
    "    if len(np.unique(y_true)) == 5:\n",
    "        classes = [6, 16, 53, 65, 92]\n",
    "        class_weight = {6: 1, 16: 1, 53: 1, 65: 1, 92: 1}\n",
    "        \n",
    "    # Out of Galaxy Case\n",
    "    if len(np.unique(y_true)) == 9:\n",
    "        classes = [15, 42, 52, 62, 64, 67, 88, 90, 95]\n",
    "        class_weight = {15: 2, 42: 1, 52: 1, 62: 1, 64: 2, 67: 1, 88: 1, 90: 1, 95: 1}\n",
    "        \n",
    "        \n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboon\n",
    "    \n",
    "    if len(np.unique(y_true)) == 14:\n",
    "        classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "        class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "#     if len(np.unique(y_true)) > 14:\n",
    "#         classes.append(99)\n",
    "#         class_weight[99] = 2\n",
    "    \n",
    "    # Galaxy Case\n",
    "    if len(np.unique(y_true)) == 5:\n",
    "        classes = [6, 16, 53, 65, 92]\n",
    "        class_weight = {6: 1, 16: 1, 53: 1, 65: 1, 92: 1}\n",
    "        \n",
    "    # Out of Galaxy Case\n",
    "    if len(np.unique(y_true)) == 9:\n",
    "        classes = [15, 42, 52, 62, 64, 67, 88, 90, 95]\n",
    "        class_weight = {15: 2, 42: 1, 52: 1, 62: 1, 64: 2, 67: 1, 88: 1, 90: 1, 95: 1}\n",
    "    \n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def set_df(arr, col_names):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.columns = col_names\n",
    "    return df\n",
    "    \n",
    "\n",
    "def predict_chunk(df_, clfs_, meta_, features, train_mean):\n",
    "    \n",
    "    print('Chunk size',df_.shape[0])\n",
    "    \n",
    "\n",
    "    full_test_in_gal, full_test_out_gal = fabriquer_feat(df_, meta_)\n",
    "        \n",
    "\n",
    "\n",
    "    in_classes = [6, 16, 53, 65, 92]\n",
    "    out_classes = [15, 42, 52, 62, 64, 67, 88, 90, 95] \n",
    "\n",
    "    if full_test_in_gal.shape[0] == 0:\n",
    "        in_df = pd.DataFrame(columns=in_classes)\n",
    "        in_ids = []\n",
    "        \n",
    "    \n",
    "    else :\n",
    "        in_ids = full_test_in_gal['object_id'].astype(np.int64).values\n",
    "        \n",
    "        del full_test_in_gal['object_id']\n",
    "        df_mean = full_test_in_gal.mean(axis=0)\n",
    "        full_test_in_gal.fillna(df_mean, inplace=True)\n",
    "    \n",
    "        # Make predictions in galaxy\n",
    "        preds_in_gal = None\n",
    "        for clf in clfs_[0]:\n",
    "            if preds_in_gal is None:\n",
    "                preds_in_gal = clf.predict_proba(full_test_in_gal[features[0]]) / len(clfs_[0])\n",
    "            else:\n",
    "                preds_in_gal += clf.predict_proba(full_test_in_gal[features[0]]) / len(clfs_[0])\n",
    "        \n",
    "        in_df = set_df(preds_in_gal, in_classes)\n",
    "        \n",
    "            \n",
    "\n",
    "    if full_test_out_gal.shape[0] == 0:\n",
    "        out_df = pd.DataFrame(columns=out_classes)    \n",
    "        out_ids = []\n",
    "    \n",
    "    else :\n",
    "        out_ids = full_test_out_gal['object_id'].astype(np.int64).values\n",
    "        \n",
    "        del full_test_out_gal['object_id']\n",
    "        df_mean = full_test_out_gal.mean(axis=0)\n",
    "        full_test_out_gal.fillna(df_mean, inplace=True)\n",
    "                \n",
    "        # Make predictions out of galaxy\n",
    "        preds_out_gal = None\n",
    "        for clf in clfs_[1]:\n",
    "            if preds_out_gal is None:\n",
    "                preds_out_gal = clf.predict_proba(full_test_out_gal[features[1]]) / len(clfs_[1])\n",
    "            else:\n",
    "                preds_out_gal += clf.predict_proba(full_test_out_gal[features[1]]) / len(clfs_[1])\n",
    "        \n",
    "        out_df = set_df(preds_out_gal, out_classes)\n",
    "        \n",
    "    # Merge predictions\n",
    "    in_out_df = pd.concat([in_df, out_df], axis=0).fillna(0)\n",
    "    print(in_out_df.shape)\n",
    "    \n",
    "    preds_ = in_out_df.values\n",
    "    \n",
    "            \n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    \n",
    "#     preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in classes])\n",
    "    preds_df_['object_id'] = np.concatenate((in_ids,out_ids), axis=0)\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "\n",
    "    print(preds_df_['class_99'].mean())\n",
    "\n",
    "    del full_test_in_gal, full_test_out_gal, preds_, in_out_df\n",
    "    gc.collect()\n",
    "\n",
    "    return preds_df_\n",
    "\n",
    "\n",
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.barplot(x='gain', y='feature', data=importances_.sort_values('mean_gain', ascending=False))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importances.png')\n",
    "\n",
    "\n",
    "def train_classifiers(full_train=None, y=None):\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 14,\n",
    "        'metric': 'multi_logloss',\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': .9,\n",
    "        'colsample_bytree': .7,\n",
    "        'reg_alpha': .01,\n",
    "        'reg_lambda': .01,\n",
    "        'min_split_gain': 0.01,\n",
    "        'min_child_weight': 10,\n",
    "        'n_estimators': 1500,\n",
    "        'silent': -1,\n",
    "        'verbose': -1,\n",
    "        'max_depth': 3\n",
    "    }\n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        get_logger().info(multi_weighted_logloss(val_y, clf.predict_proba(val_x, num_iteration=clf.best_iteration_)))\n",
    "\n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df['feature'] = full_train.columns\n",
    "        imp_df['gain'] = clf.feature_importances_\n",
    "        imp_df['fold'] = fold_ + 1\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "        clfs.append(clf)\n",
    "\n",
    "    get_logger().info('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "    print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "\n",
    "    return clfs, importances, oof_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_new_columns(aggs):\n",
    "    return [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def add_band_feats(df, db):\n",
    "    \n",
    "    \n",
    "### 均值\n",
    "    print('Adding feats for the flux mean per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_mean' for col in stats.columns.tolist()]\n",
    "    \n",
    "    # band_#_flux_mean互相减去\n",
    "    mean_cols = stats.columns.tolist()\n",
    "    for col in mean_cols:\n",
    "        subtract_cols = [col_ for col_ in mean_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "      \n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "### 标准差    \n",
    "    print('Adding feats for the flux std per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].std().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_std' for col in stats.columns.tolist()]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "\n",
    "    \n",
    "### 偏度    \n",
    "    print('Adding feats for the flux skew per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_skew' for col in stats.columns.tolist()]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "### 最大值\n",
    "    print('Adding feats for the flux max per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].max().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_max' for col in stats.columns.tolist()]\n",
    "    # band_#_flux_max互相减去\n",
    "    max_cols = stats.columns.tolist()\n",
    "    for col in max_cols:\n",
    "        subtract_cols = [col_ for col_ in max_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "            \n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "### 最小值    \n",
    "    print('Adding feats for the flux min per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].min().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_min' for col in stats.columns.tolist()]\n",
    "    # band_#_flux_min互相做差\n",
    "    min_cols = stats.columns.tolist()\n",
    "    for col in min_cols:\n",
    "        subtract_cols = [col_ for col_ in min_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# ### ......  Mean\n",
    "#     print('Adding feats for the flux_err mean per band...')\n",
    "#     stats = db.groupby(['object_id','passband'])['flux_err'].mean().unstack()\n",
    "#     stats.columns = ['band_' + str(col) + '_flux_err_mean' for col in stats.columns.tolist()]      \n",
    "#     print('Feats added:',stats.columns.tolist())\n",
    "#     stats['object_id'] = stats.index    \n",
    "#     df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### 遍历band_list计算 \n",
    "    print('Adding feats for the flux (max-min)/mean per band...')\n",
    "    for band_n in range(6):\n",
    "        df['band_' + str(band_n) + '_flux_diff1'] = df['band_' + str(band_n) + '_flux_max'] - df['band_' + str(band_n) + '_flux_min']\n",
    "        df['band_' + str(band_n) + '_flux_diff2'] = df['band_' + str(band_n) + '_flux_diff1']/df['band_' + str(band_n) + '_flux_mean']\n",
    "        print('Feature added: band_' + str(band_n) + '_flux_diff2')\n",
    "        \n",
    "#         df['band_' + str(band_n) + '_flux_err_ratio'] = df['band_' + str(band_n) + '_flux_err_mean']/df['band_' + str(band_n) + '_flux_mean']\n",
    "#         print('Feature added: band_' + str(band_n) + '_flux_err_ratio')\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def add_feats_within_time_interval_out(int_n, df, db):\n",
    "    print('Number of Intervals :', int_n)\n",
    "    t_min = db.mjd.min()\n",
    "    t_max = db.mjd.max()\n",
    "    print('Min and Max MJD time : {}, {}'.format(t_min, t_max))    \n",
    "    int_dur = (t_max - t_min)/int_n\n",
    "    for i in range(int_n):\n",
    "        \n",
    "        db_fil = db[(db.mjd>=(t_min+i*int_dur))&(db.mjd<(t_min + (i+1)*int_dur))][['object_id','flux','passband']]\n",
    "        print('Interval #{}, record quantity: {}'.format(i+1, db_fil.shape[0]))\n",
    "        \n",
    "        # interval_#_flux_？\n",
    "        stats = db_fil.groupby('object_id', as_index=False)['flux'].agg({'interval_{}_flux_mean'.format(i+1):'mean',\n",
    "#                                                                           'interval_{}_flux_std'.format(i+1):'std',\n",
    "                                                                          'interval_{}_flux_min'.format(i+1):'min',\n",
    "                                                                          'interval_{}_flux_max'.format(i+1):'max',\n",
    "#                                                                          'interval_{}_flux_skew'.format(i+1):'skew'\n",
    "                                                                        })\n",
    "        print('New features added: ',stats.columns.tolist())\n",
    "        df = df.merge(stats, on='object_id', how='left')\n",
    "        \n",
    "        \n",
    "#         # interval_#_band_#_flux_？\n",
    "#         stats = db_fil.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "#         stats.columns = ['interval_{}_band_{}_flux_skew'.format(i+1, str(col)) for col in stats.columns.tolist()]\n",
    "#         print('Feats added:',stats.columns.tolist())\n",
    "#         stats['object_id'] = stats.index    \n",
    "#         df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "        \n",
    "                \n",
    "#     # interval_#_flux_？ 互相做差\n",
    "#     for key in ['max', 'min', 'mean']:\n",
    "# #     for key in ['mean']:\n",
    "#         key_cols = ['interval_{}_flux_{}'.format(i, key) for i in range(1, int_n+1)]\n",
    "#         for col in key_cols:\n",
    "#             subtract_cols = [col_ for col_ in key_cols if col_ < col]\n",
    "#             for sub_col in subtract_cols:\n",
    "#                 df['{}_minus_{}'.format(col, sub_col)] = df[col] - df[sub_col]\n",
    "#                 print('Feature added:', '{}_minus_{}'.format(col, sub_col))\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Dimension of data after adding features relevant to time intervals', df.shape)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def add_feats_within_time_interval(int_n, df, db):\n",
    "    print('Number of Intervals :', int_n)\n",
    "    t_min = db.mjd.min()\n",
    "    t_max = db.mjd.max()\n",
    "    print('Min and Max MJD time : {}, {}'.format(t_min, t_max))    \n",
    "    int_dur = (t_max - t_min)/int_n\n",
    "    for i in range(int_n):\n",
    "        \n",
    "        db_fil = db[(db.mjd>=(t_min+i*int_dur))&(db.mjd<(t_min + (i+1)*int_dur))][['object_id','flux','passband']]\n",
    "        print('Interval #{}, record quantity: {}'.format(i+1, db_fil.shape[0]))\n",
    "        \n",
    "        # interval_#_flux_？\n",
    "        stats = db_fil.groupby('object_id', as_index=False)['flux'].agg({'interval_{}_flux_mean'.format(i+1):'mean',\n",
    "                                                                          'interval_{}_flux_std'.format(i+1):'std',\n",
    "                                                                          'interval_{}_flux_min'.format(i+1):'min',\n",
    "                                                                          'interval_{}_flux_max'.format(i+1):'max',\n",
    "                                                                         'interval_{}_flux_skew'.format(i+1):'skew'})\n",
    "        print('New features added: ',stats.columns.tolist())\n",
    "        df = df.merge(stats, on='object_id', how='left')\n",
    "        \n",
    "        \n",
    "#         # interval_#_band_#_flux_？\n",
    "#         stats = db_fil.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "#         stats.columns = ['interval_{}_band_{}_flux_skew'.format(i+1, str(col)) for col in stats.columns.tolist()]\n",
    "#         print('Feats added:',stats.columns.tolist())\n",
    "#         stats['object_id'] = stats.index    \n",
    "#         df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "        \n",
    "                \n",
    "    # interval_#_flux_？ 互相做差\n",
    "    for key in ['max', 'min', 'mean']:\n",
    "#     for key in ['max']:\n",
    "        key_cols = ['interval_{}_flux_{}'.format(i, key) for i in range(1, int_n+1)]\n",
    "        for col in key_cols:\n",
    "            subtract_cols = [col_ for col_ in key_cols if col_ < col]\n",
    "            for sub_col in subtract_cols:\n",
    "                df['{}_minus_{}'.format(col, sub_col)] = df[col] - df[sub_col]\n",
    "                print('Feature added:', '{}_minus_{}'.format(col, sub_col))\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Dimension of data after adding features relevant to time intervals', df.shape)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def agg_by_flux_feats(df):\n",
    "    \n",
    "    df['flux_ratio'] = df['flux'] / df['flux_err']\n",
    "    \n",
    "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    \n",
    "    aggs = {\n",
    "#         'mjd': ['min', 'max', 'size'],\n",
    "#         'passband': ['mean', 'std', 'var'],  \n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'flux_ratio': ['min', 'max', 'mean', 'std'],\n",
    "        'detected': ['mean'],  # ''min', 'max', 'mean', 'median', 'std'],\n",
    "    }   \n",
    "\n",
    "#     aggs['flux_ratio_sq'] = ['sum']\n",
    "#     aggs['flux_by_flux_ratio_sq'] = ['sum']\n",
    "\n",
    "    \n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = get_new_columns(aggs)\n",
    "    agg_df.columns = new_columns\n",
    "\n",
    "    agg_df = add_flux_second_order_features_to_agg(df=agg_df)\n",
    "    \n",
    "    return agg_df\n",
    "    \n",
    "\n",
    "def get_by_galaxy(df):\n",
    "    df_in_gal = df[df['in_galaxy']==1]\n",
    "    objects_in_gal = df_in_gal['object_id'].unique().tolist()\n",
    "    print('Number of objects in galaxy :',len(objects_in_gal))\n",
    "    \n",
    "    df_out_gal = df[df['in_galaxy']==0]\n",
    "    objects_out_gal = df_out_gal['object_id'].unique().tolist()\n",
    "    print('Number of objects out of galaxy :',len(objects_out_gal))\n",
    "    print('Just to check, sum of objects :', len(objects_in_gal) + len(objects_out_gal))\n",
    "    print('Total number should be', len(df['object_id'].unique().tolist()))\n",
    "    \n",
    "    return df_in_gal, df_out_gal\n",
    "    \n",
    "def add_photo_feats(df):\n",
    "    df['hostgal_photoz_ratio'] = df['hostgal_photoz']/df['hostgal_photoz_err']\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def fabriquer_feat(db, meta):\n",
    "    \n",
    "\n",
    "    \n",
    "    # META数据提供划分银河系内外的依据\n",
    "    # 增加是否属于银河系的特征\n",
    "    meta.distmod.fillna(0,inplace=True)\n",
    "    meta['in_galaxy'] = 0\n",
    "    meta.loc[(meta.distmod == 0), 'in_galaxy'] = 1\n",
    "    \n",
    "    # 时序数据和META数据融合，形成以mjd为行的数据\n",
    "    db_meta = db.merge(meta, on='object_id', how='left')\n",
    "    print('Dimension of merge data for MJD relevant data and META data ', db_meta.shape)\n",
    "    \n",
    "    # 对时序融合数据进行分割\n",
    "    db_in_gal, db_out_gal = get_by_galaxy(db_meta)\n",
    "    print('Dimension of merge data for that in galaxy and that out of galaxy ', db_in_gal.shape, db_out_gal.shape)\n",
    "    \n",
    "    # 基本特征聚合\n",
    "    agg_df = agg_by_flux_feats(db)\n",
    "    print('Dimension of aggregated data on flux features', agg_df.shape)\n",
    "    \n",
    "    # 聚合数据和META数据融合，形成以object_id为行的数据\n",
    "    agg_df_meta = agg_df.merge(meta, on='object_id', how='left')\n",
    "    print('Dimension of merge data for Object relevant data and META data', agg_df_meta.shape)\n",
    "\n",
    "    # 对object融合数据进行分割\n",
    "    df_in_gal, df_out_gal = get_by_galaxy(agg_df_meta)\n",
    "    print('Dimension of merge data for that in galaxy and that out of galaxy ', df_in_gal.shape, df_out_gal.shape)\n",
    "\n",
    "    # 对银河系内外数据分别提取特征\n",
    "    print('Features extraction begins...')\n",
    "    \n",
    "    # 特征提取前先校验数据行数\n",
    "    if df_in_gal.shape[0] == 0:\n",
    "        print('Object relevant data in the Galaxy has no data, nothing to predict.')\n",
    "    \n",
    "    else :\n",
    "        # 银河系内\n",
    "        print('In terms of that in the Galaxy...')\n",
    "        \n",
    "        # 增加band相关特征\n",
    "        df_in_gal = add_band_feats(df_in_gal, db_in_gal)    \n",
    "        \n",
    "        # 增加按MJD划分时间统计得到的特征\n",
    "        df_in_gal = add_feats_within_time_interval(6, df_in_gal, db_in_gal)        \n",
    "    \n",
    "    if df_out_gal.shape[0] == 0:\n",
    "        print('Object relevant data out of the Galaxy has no data, nothing to predict.')    \n",
    "    \n",
    "    else :       \n",
    "        # 银河系外\n",
    "        print('In terms of that out of the Galaxy...')\n",
    "        \n",
    "        # 增加hostgal_photoz相关特征\n",
    "        df_out_gal = add_photo_feats(df_out_gal)\n",
    "        \n",
    "        # 增加band相关特征\n",
    "        df_out_gal = add_band_feats(df_out_gal, db_out_gal)    \n",
    "        \n",
    "        # 增加按MJD划分时间统计得到的特征\n",
    "        df_out_gal = add_feats_within_time_interval_out(6, df_out_gal, db_out_gal) \n",
    "    \n",
    "    return df_in_gal, df_out_gal\n",
    "    \n",
    "    \n",
    "\n",
    "def add_flux_second_order_features_to_agg(df):\n",
    "#     df['mjd_diff'] = df['mjd_max'] - df['mjd_min']\n",
    "    df['flux_diff'] = df['flux_max'] - df['flux_min']\n",
    "    df['flux_dif2'] = (df['flux_max'] - df['flux_min']) / df['flux_mean']\n",
    "#     df['flux_w_mean'] = df['flux_by_flux_ratio_sq_sum'] / df['flux_ratio_sq_sum']\n",
    "#     df['flux_dif3'] = (df['flux_max'] - df['flux_min']) / df['flux_w_mean']\n",
    "\n",
    "#     del df['mjd_max'], df['mjd_min']\n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "\n",
    "    train = pd.read_csv('../input/training_set.csv')\n",
    "\n",
    "    meta_train = pd.read_csv('../input/training_set_metadata.csv')\n",
    "    # 去除无效特征\n",
    "    del meta_train['hostgal_specz']\n",
    "    print('Feature hostgal_specz is removed')\n",
    "    \n",
    "    \n",
    "    full_train_in_gal, full_train_out_gal = fabriquer_feat(train, meta_train)\n",
    "    \n",
    "    del train\n",
    "    \n",
    "    print('Training begins...')\n",
    "    \n",
    "\n",
    "    y_list = []\n",
    "    preds_list = []\n",
    "    clf_list = []\n",
    "    for df in [full_train_in_gal, full_train_out_gal]:\n",
    "        del df['object_id']\n",
    "        df_mean = df.mean(axis=0)\n",
    "        df.fillna(df_mean, inplace=True)\n",
    "        y = df['target']\n",
    "        y_list.append(y)\n",
    "        del df['target']\n",
    "        gc.collect()\n",
    "        get_logger().info(df.columns)\n",
    "    \n",
    "        clfs, importances, preds = train_classifiers(df, y)\n",
    "        save_importances(importances_=importances)\n",
    "        preds_list.append(preds)\n",
    "        clf_list.append(clfs)\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    meta_test = pd.read_csv('../input/test_set_metadata.csv')\n",
    "    # 去除无效特征\n",
    "    del meta_test['hostgal_specz']\n",
    "    print('Feature hostgal_specz is removed')\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "#     chunks = 5000000\n",
    "    chunks = 500000\n",
    "    remain_df = None\n",
    "    \n",
    "    for i_c, df in enumerate(pd.read_csv('../input/test_set_sample.csv', chunksize=chunks, iterator=True)):\n",
    "        # Check object_ids\n",
    "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "        unique_ids = np.unique(df['object_id'])\n",
    "        # 最后一个ID的内容\n",
    "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "    \n",
    "        if remain_df is None:\n",
    "            #  除最后一个ID外的内容\n",
    "            df = df.loc[df['object_id'].isin(unique_ids[:-1])].copy()\n",
    "        else:\n",
    "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "    \n",
    "        # Create remaining samples df\n",
    "        remain_df = new_remain_df\n",
    "        \n",
    "        \n",
    "    \n",
    "        preds_df = predict_chunk(df_=df,\n",
    "                                 clfs_=clf_list,\n",
    "                                 meta_=meta_test,\n",
    "                                 features=[full_train_in_gal.columns, full_train_out_gal.columns],\n",
    "                                 train_mean=None)\n",
    "    \n",
    "        if i_c == 0:\n",
    "            preds_df.to_csv('predictions_v3.csv', header=True, index=False, float_format='%.6f')\n",
    "        else:\n",
    "            preds_df.to_csv('predictions_v3.csv', header=False, mode='a', index=False, float_format='%.6f')\n",
    "    \n",
    "        del preds_df\n",
    "        gc.collect()\n",
    "    \n",
    "        if (i_c + 1) % 5 == 0:\n",
    "            # get_logger().info('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "            # print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "            get_logger().info('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "            get_logger().info('Progress percentage : %5.2f' % (chunks * (i_c + 1)/500000000))\n",
    "            get_logger().info('Time estimated left : %5.2f' % ((time.time() - start) / 60 * (500000000-chunks * (i_c + 1))/(chunks * (i_c + 1))))\n",
    "            print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "            print('Progress percentage : %5.2f' % (chunks * (i_c + 1)/500000000))\n",
    "            print('Time estimated left : %5.2f' % ((time.time() - start) / 60 * (500000000-chunks * (i_c + 1))/(chunks * (i_c + 1))))\n",
    "    \n",
    "    # Compute last object in remain_df\n",
    "    \n",
    "    preds_df = predict_chunk(df_=remain_df,\n",
    "                             clfs_=clf_list,\n",
    "                             meta_=meta_test,\n",
    "                             features=[full_train_in_gal.columns, full_train_out_gal.columns],\n",
    "                             train_mean=None)\n",
    "    \n",
    "    preds_df.to_csv('predictions_v3.csv', header=False, mode='a', index=False, float_format='%.6f')\n",
    "    \n",
    "    z = pd.read_csv('predictions_v3.csv')\n",
    "    \n",
    "    z = z.groupby('object_id').mean()\n",
    "    \n",
    "    z.to_csv('single_predictions_v3.csv', index=True, float_format='%.6f')\n",
    "    \n",
    "    z = z.astype(np.float32)\n",
    "    \n",
    "    z['object_id'] = z.index.astype(np.int32)\n",
    "    \n",
    "    z = z.drop_duplicates(subset=['object_id'], keep='first')\n",
    "\n",
    "    z.to_csv('single_predictions_v3.gz', index=False, float_format='%.6f', compression='gzip')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gc.enable()\n",
    "    create_logger()\n",
    "    try:\n",
    "        main()\n",
    "    except Exception:\n",
    "        get_logger().exception('Unexpected Exception Occured')\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
