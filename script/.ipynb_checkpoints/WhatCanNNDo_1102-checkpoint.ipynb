{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import lightgbm as lgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_ohe, y_p):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "#     classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "#     class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "\n",
    "    print('Number of classes :', len(y_ohe[0]))\n",
    "    \n",
    "    if len(y_ohe[0]) == 14:\n",
    "        classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "        class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    \n",
    "    # Galaxy Case\n",
    "    if len(y_ohe[0]) == 5:\n",
    "        classes = [6, 16, 53, 65, 92]\n",
    "        class_weight = {6: 1, 16: 1, 53: 1, 65: 1, 92: 1}\n",
    "        \n",
    "    # Out of Galaxy Case\n",
    "    if len(y_ohe[0]) == 9:\n",
    "        classes = [15, 42, 52, 62, 64, 67, 88, 90, 95]\n",
    "        class_weight = {15: 2, 42: 1, 52: 1, 62: 1, 64: 2, 67: 1, 88: 1, 90: 1, 95: 1}\n",
    "        \n",
    "        \n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_df(arr, col_names):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.columns = col_names\n",
    "    return df\n",
    "\n",
    "def get_new_columns(aggs):\n",
    "    return [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "def agg_by_flux_feats(df):\n",
    "    \n",
    "    df['flux_ratio'] = df['flux'] / df['flux_err']\n",
    "    \n",
    "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    \n",
    "    aggs = {\n",
    "#         'mjd': ['min', 'max', 'size'],\n",
    "#         'passband': ['mean', 'std', 'var'],  \n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'flux_ratio': ['min', 'max', 'mean', 'std'],\n",
    "        'detected': ['mean'],  # ''min', 'max', 'mean', 'median', 'std'],\n",
    "    }   \n",
    "\n",
    "#     aggs['flux_ratio_sq'] = ['sum']\n",
    "#     aggs['flux_by_flux_ratio_sq'] = ['sum']\n",
    "\n",
    "    \n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = get_new_columns(aggs)\n",
    "    agg_df.columns = new_columns\n",
    "\n",
    "    agg_df = add_flux_second_order_features_to_agg(df=agg_df)\n",
    "    \n",
    "    return agg_df\n",
    "\n",
    "def add_flux_second_order_features_to_agg(df):\n",
    "#     df['mjd_diff'] = df['mjd_max'] - df['mjd_min']\n",
    "    df['flux_diff'] = df['flux_max'] - df['flux_min']\n",
    "    df['flux_dif2'] = (df['flux_max'] - df['flux_min']) / df['flux_mean']\n",
    "#     df['flux_w_mean'] = df['flux_by_flux_ratio_sq_sum'] / df['flux_ratio_sq_sum']\n",
    "#     df['flux_dif3'] = (df['flux_max'] - df['flux_min']) / df['flux_w_mean']\n",
    "\n",
    "#     del df['mjd_max'], df['mjd_min']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_by_galaxy(df):\n",
    "    df_in_gal = df[df['in_galaxy']==1]\n",
    "    objects_in_gal = df_in_gal['object_id'].unique().tolist()\n",
    "    print('Number of objects in galaxy :',len(objects_in_gal))\n",
    "    \n",
    "    df_out_gal = df[df['in_galaxy']==0]\n",
    "    objects_out_gal = df_out_gal['object_id'].unique().tolist()\n",
    "    print('Number of objects out of galaxy :',len(objects_out_gal))\n",
    "    print('Just to check, sum of objects :', len(objects_in_gal) + len(objects_out_gal))\n",
    "    print('Total number should be', len(df['object_id'].unique().tolist()))\n",
    "    return df_in_gal, df_out_gal\n",
    "\n",
    "\n",
    "def add_feats_within_time_interval_out(int_n, df, db):\n",
    "    print('Number of Intervals :', int_n)\n",
    "    t_min = db.mjd.min()\n",
    "    t_max = db.mjd.max()\n",
    "    print('Min and Max MJD time : {}, {}'.format(t_min, t_max))    \n",
    "    int_dur = (t_max - t_min)/int_n\n",
    "    for i in range(int_n):\n",
    "        \n",
    "        db_fil = db[(db.mjd>=(t_min+i*int_dur))&(db.mjd<(t_min + (i+1)*int_dur))][['object_id','flux','passband']]\n",
    "        print('Interval #{}, record quantity: {}'.format(i+1, db_fil.shape[0]))\n",
    "        \n",
    "        # interval_#_flux_？\n",
    "        stats = db_fil.groupby('object_id', as_index=False)['flux'].agg({'interval_{}_flux_mean'.format(i+1):'mean',\n",
    "#                                                                           'interval_{}_flux_std'.format(i+1):'std',\n",
    "                                                                          'interval_{}_flux_min'.format(i+1):'min',\n",
    "                                                                          'interval_{}_flux_max'.format(i+1):'max',\n",
    "#                                                                          'interval_{}_flux_skew'.format(i+1):'skew'\n",
    "                                                                        })\n",
    "#         print('New features added: ',stats.columns.tolist())\n",
    "        df = df.merge(stats, on='object_id', how='left')\n",
    "        \n",
    "# 仅仅看看神经网络在多特征情况下表现如何         \n",
    "        # interval_#_band_#_flux_？\n",
    "        stats = db_fil.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "        stats.columns = ['interval_{}_band_{}_flux_skew'.format(i+1, str(col)) for col in stats.columns.tolist()]\n",
    "        print('Feats added:',stats.columns.tolist())\n",
    "        stats['object_id'] = stats.index    \n",
    "        df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "# 仅仅看看神经网络在多特征情况下表现如何         \n",
    "                \n",
    "    # interval_#_flux_？ 互相做差\n",
    "    for key in ['max', 'min', 'mean']:\n",
    "#     for key in ['mean']:\n",
    "        key_cols = ['interval_{}_flux_{}'.format(i, key) for i in range(1, int_n+1)]\n",
    "        for col in key_cols:\n",
    "            subtract_cols = [col_ for col_ in key_cols if col_ < col]\n",
    "            for sub_col in subtract_cols:\n",
    "                df['{}_minus_{}'.format(col, sub_col)] = df[col] - df[sub_col]\n",
    "                print('Feature added:', '{}_minus_{}'.format(col, sub_col))\n",
    "        \n",
    "\n",
    "    \n",
    "    print('Dimension of data after adding features relevant to time intervals', df.shape)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_feats_within_time_interval(int_n, df, db):\n",
    "    print('Number of Intervals :', int_n)\n",
    "    t_min = db.mjd.min()\n",
    "    t_max = db.mjd.max()\n",
    "    print('Min and Max MJD time : {}, {}'.format(t_min, t_max))    \n",
    "    int_dur = (t_max - t_min)/int_n\n",
    "    for i in range(int_n):\n",
    "        \n",
    "        db_fil = db[(db.mjd>=(t_min+i*int_dur))&(db.mjd<(t_min + (i+1)*int_dur))][['object_id','flux','passband']]\n",
    "        print('Interval #{}, record quantity: {}'.format(i+1, db_fil.shape[0]))\n",
    "        \n",
    "        # interval_#_flux_？\n",
    "        stats = db_fil.groupby('object_id', as_index=False)['flux'].agg({'interval_{}_flux_mean'.format(i+1):'mean',\n",
    "                                                                          'interval_{}_flux_std'.format(i+1):'std',\n",
    "                                                                          'interval_{}_flux_min'.format(i+1):'min',\n",
    "                                                                          'interval_{}_flux_max'.format(i+1):'max',\n",
    "                                                                         'interval_{}_flux_skew'.format(i+1):'skew'})\n",
    "#         print('New features added: ',stats.columns.tolist())\n",
    "        df = df.merge(stats, on='object_id', how='left')\n",
    "        \n",
    "# 仅仅看看神经网络在多特征情况下表现如何        \n",
    "        # interval_#_band_#_flux_？\n",
    "        stats = db_fil.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "        stats.columns = ['interval_{}_band_{}_flux_skew'.format(i+1, str(col)) for col in stats.columns.tolist()]\n",
    "        print('Feats added:',stats.columns.tolist())\n",
    "        stats['object_id'] = stats.index    \n",
    "        df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "# 仅仅看看神经网络在多特征情况下表现如何         \n",
    "                \n",
    "    # interval_#_flux_？ 互相做差\n",
    "    for key in ['max', 'min', 'mean']:\n",
    "#     for key in ['max']:\n",
    "        key_cols = ['interval_{}_flux_{}'.format(i, key) for i in range(1, int_n+1)]\n",
    "        for col in key_cols:\n",
    "            subtract_cols = [col_ for col_ in key_cols if col_ < col]\n",
    "            for sub_col in subtract_cols:\n",
    "                df['{}_minus_{}'.format(col, sub_col)] = df[col] - df[sub_col]\n",
    "#                 print('Feature added:', '{}_minus_{}'.format(col, sub_col))\n",
    "         \n",
    "\n",
    "    \n",
    "    print('Dimension of data after adding features relevant to time intervals', df.shape)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_band_feats(df, db):\n",
    "    \n",
    "    \n",
    "### 均值\n",
    "    print('Adding feats for the flux mean per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_mean' for col in stats.columns.tolist()]\n",
    "    \n",
    "    # band_#_flux_mean互相减去\n",
    "    mean_cols = stats.columns.tolist()\n",
    "    for col in mean_cols:\n",
    "        subtract_cols = [col_ for col_ in mean_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "      \n",
    "    # print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "### 标准差    \n",
    "    print('Adding feats for the flux std per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].std().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_std' for col in stats.columns.tolist()]\n",
    "    # print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "\n",
    "    \n",
    "### 偏度    \n",
    "    print('Adding feats for the flux skew per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_skew' for col in stats.columns.tolist()]\n",
    "    # print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "### 最大值\n",
    "    print('Adding feats for the flux max per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].max().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_max' for col in stats.columns.tolist()]\n",
    "    # band_#_flux_max互相减去\n",
    "    max_cols = stats.columns.tolist()\n",
    "    for col in max_cols:\n",
    "        subtract_cols = [col_ for col_ in max_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "            \n",
    "    # print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "### 最小值    \n",
    "    print('Adding feats for the flux min per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].min().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_min' for col in stats.columns.tolist()]\n",
    "    \n",
    "#     # band_#_flux_min互相做差\n",
    "#     min_cols = stats.columns.tolist()\n",
    "#     for col in min_cols:\n",
    "#         subtract_cols = [col_ for col_ in min_cols if col_ < col]\n",
    "#         for sub_col in subtract_cols:\n",
    "#             stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "            \n",
    "    # print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# ### ......  Mean\n",
    "#     print('Adding feats for the flux_err mean per band...')\n",
    "#     stats = db.groupby(['object_id','passband'])['flux_err'].mean().unstack()\n",
    "#     stats.columns = ['band_' + str(col) + '_flux_err_mean' for col in stats.columns.tolist()]      \n",
    "#     print('Feats added:',stats.columns.tolist())\n",
    "#     stats['object_id'] = stats.index    \n",
    "#     df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### 遍历band_list计算 \n",
    "    print('Adding feats for the flux (max-min)/mean per band...')\n",
    "    for band_n in range(6):\n",
    "        df['band_' + str(band_n) + '_flux_diff1'] = df['band_' + str(band_n) + '_flux_max'] - df['band_' + str(band_n) + '_flux_min']\n",
    "        df['band_' + str(band_n) + '_flux_diff2'] = df['band_' + str(band_n) + '_flux_diff1']/df['band_' + str(band_n) + '_flux_mean']\n",
    "        # print('Feature added: band_' + str(band_n) + '_flux_diff2')\n",
    "        \n",
    "#         df['band_' + str(band_n) + '_flux_err_ratio'] = df['band_' + str(band_n) + '_flux_err_mean']/df['band_' + str(band_n) + '_flux_mean']\n",
    "#         print('Feature added: band_' + str(band_n) + '_flux_err_ratio')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    print('Dimension of data after adding features relevant to bands', df.shape)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def add_flux_second_order_features_to_agg(df):\n",
    "#     df['mjd_diff'] = df['mjd_max'] - df['mjd_min']\n",
    "    df['flux_diff'] = df['flux_max'] - df['flux_min']\n",
    "    df['flux_dif2'] = (df['flux_max'] - df['flux_min']) / df['flux_mean']\n",
    "#     df['flux_w_mean'] = df['flux_by_flux_ratio_sq_sum'] / df['flux_ratio_sq_sum']\n",
    "#     df['flux_dif3'] = (df['flux_max'] - df['flux_min']) / df['flux_w_mean']\n",
    "\n",
    "#     del df['mjd_max'], df['mjd_min']\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_photo_feats(df):\n",
    "    df['hostgal_photoz_ratio'] = df['hostgal_photoz']/df['hostgal_photoz_err']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def fabriquer_feat(db, meta):\n",
    "    \n",
    "    # # 去除无效特征\n",
    "    # del meta['hostgal_specz']\n",
    "    # print('Feature hostgal_specz is removed')\n",
    "    \n",
    "    # META数据提供划分银河系内外的依据\n",
    "    # 增加是否属于银河系的特征\n",
    "    meta.distmod.fillna(0,inplace=True)\n",
    "    meta['in_galaxy'] = 0\n",
    "    meta.loc[(meta.distmod == 0), 'in_galaxy'] = 1\n",
    "    \n",
    "    # 时序数据和META数据融合，形成以mjd为行的数据\n",
    "    db_meta = db.merge(meta, on='object_id', how='left')\n",
    "    print('Dimension of merge data for MJD relevant data and META data ', db_meta.shape)\n",
    "    \n",
    "    # 对时序融合数据进行分割\n",
    "    db_in_gal, db_out_gal = get_by_galaxy(db_meta)\n",
    "    print('Dimension of merge data for that in galaxy and that out of galaxy ', db_in_gal.shape, db_out_gal.shape)\n",
    "    \n",
    "    # 基本特征聚合\n",
    "    agg_df = agg_by_flux_feats(db)\n",
    "    print('Dimension of aggregated data on flux features', agg_df.shape)\n",
    "    \n",
    "    # 聚合数据和META数据融合，形成以object_id为行的数据\n",
    "    agg_df_meta = agg_df.merge(meta, on='object_id', how='left')\n",
    "    print('Dimension of merge data for Object relevant data and META data', agg_df_meta.shape)\n",
    "\n",
    "    # 对object融合数据进行分割\n",
    "    df_in_gal, df_out_gal = get_by_galaxy(agg_df_meta)\n",
    "    print('Dimension of merge data for that in galaxy and that out of galaxy ', df_in_gal.shape, df_out_gal.shape)\n",
    "\n",
    "    if df_in_gal.shape[0] >0:\n",
    "        # 对银河系内数据提取特征\n",
    "        print('Features extraction for objects in the Galaxy begins...')\n",
    "        \n",
    "        # 增加band相关特征\n",
    "        df_in_gal = add_band_feats(df_in_gal, db_in_gal)    \n",
    "        \n",
    "        # 增加按MJD划分时间统计得到的特征\n",
    "        df_in_gal = add_feats_within_time_interval(6, df_in_gal, db_in_gal)\n",
    "        \n",
    "    \n",
    "    if df_out_gal.shape[0] >0:\n",
    "        # 对银河系外数据提取特征\n",
    "        print('Features extraction for objects out of the Galaxy begins...')\n",
    "        \n",
    "        # 增加hostgal_photoz相关特征\n",
    "        df_out_gal = add_photo_feats(df_out_gal)\n",
    "        \n",
    "        # 增加band相关特征\n",
    "        df_out_gal = add_band_feats(df_out_gal, db_out_gal)    \n",
    "        \n",
    "        # 增加按MJD划分时间统计得到的特征\n",
    "        df_out_gal = add_feats_within_time_interval_out(6, df_out_gal, db_out_gal) \n",
    "    \n",
    "    return df_in_gal, df_out_gal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def to_cat(y):    \n",
    "    classes = sorted(np.unique(y))\n",
    "    \n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "\n",
    "    for i,val in enumerate(unique_y):\n",
    "        class_map[val] = i            \n",
    "    y_map = np.zeros((y.shape[0],))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "    y_categorical = to_categorical(y_map)    \n",
    "    \n",
    "    return y_categorical\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_by_nn(full_train, y):\n",
    "    full_train_new = full_train.copy()\n",
    "    ss = StandardScaler()\n",
    "    full_train_ss = ss.fit_transform(full_train_new)\n",
    "    \n",
    "    classes = sorted(y.unique())\n",
    "    \n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "\n",
    "    for i,val in enumerate(unique_y):\n",
    "        class_map[val] = i            \n",
    "    y_map = np.zeros((y.shape[0],))\n",
    "    y_map = np.array([class_map[val] for val in y])\n",
    "    y_categorical = to_categorical(y_map)    \n",
    "\n",
    "    \n",
    "    y_count = Counter(y_map)\n",
    "    wtable = np.zeros((len(unique_y),))\n",
    "    for i in range(len(unique_y)):\n",
    "        wtable[i] = y_count[i]/y_map.shape[0]    \n",
    "    \n",
    "    \n",
    "    def mywloss(y_true, y_pred):  \n",
    "  \n",
    "        yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "        loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "        return loss\n",
    "    \n",
    "#     K.clear_session()\n",
    "    def build_model(dropout_rate=0.25,activation='relu'):\n",
    "        start_neurons = 512\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(Dense(start_neurons//2,activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(Dense(start_neurons//4,activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(Dense(start_neurons//8,activation=activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate/2))\n",
    "        \n",
    "        model.add(Dense(len(classes), activation='softmax'))\n",
    "        return model    \n",
    "    \n",
    "        \n",
    "    clfs = []\n",
    "    oof_preds = np.zeros((len(full_train_ss), len(classes)))\n",
    "    epochs = 600\n",
    "    batch_size = 100\n",
    "    checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "        x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n",
    "        x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n",
    "        \n",
    "        model = build_model(dropout_rate=0.5,activation='tanh')    \n",
    "        model.compile(loss=mywloss, optimizer='adam', metrics=['accuracy'])\n",
    "        history = model.fit(x_train, y_train,\n",
    "                        validation_data=[x_valid, y_valid], \n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n",
    "        \n",
    "        plot_loss_acc(history)\n",
    "        \n",
    "        print('Loading Best Model')\n",
    "        model.load_weights('./keras.model')\n",
    "        # # Get predicted probabilities for each class\n",
    "        oof_preds[val_, :] = model.predict_proba(x_valid,batch_size=batch_size)\n",
    "        print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid,batch_size=batch_size)))\n",
    "        clfs.append(model)\n",
    "    \n",
    "    return ss, y_categorical, oof_preds, clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig(r'../feat/confusion_matrix_{}.pdf'.format(datetime.datetime.now().strftime('%m%d_%H%M')))\n",
    "    \n",
    "    \n",
    "\n",
    "def get_confusion_matrix(y, preds):\n",
    "    unique_y = np.unique(y)\n",
    "    class_map = dict()\n",
    "    for i,val in enumerate(unique_y):\n",
    "        class_map[val] = i\n",
    "            \n",
    "#     y_map = np.zeros((y.shape[0],))\n",
    "    y_map = np.array([class_map[val] for val in y]) \n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_map, np.argmax(preds, axis=-1))  \n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    \n",
    "    sample_sub = pd.read_csv('../input/sample_submission.csv')\n",
    "    class_names = list(sample_sub.columns[1:-1])\n",
    "    del sample_sub;gc.collect()\n",
    "    \n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    foo = plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "\n",
    "train = pd.read_csv('../input/training_set.csv')\n",
    "\n",
    "meta_train = pd.read_csv('../input/training_set_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature hostgal_specz is removed\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fabriquer_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a15b0797d5a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feature hostgal_specz is removed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfull_train_in_gal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_train_out_gal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfabriquer_feat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfull_train_in_gal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_train_out_gal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fabriquer_feat' is not defined"
     ]
    }
   ],
   "source": [
    "# 去除无效特征\n",
    "del meta_train['hostgal_specz']\n",
    "print('Feature hostgal_specz is removed')\n",
    "\n",
    "full_train_in_gal, full_train_out_gal = fabriquer_feat(train, meta_train)\n",
    "\n",
    "for df in [full_train_in_gal, full_train_out_gal]:\n",
    "    del df['object_id']\n",
    "    df_mean = df.mean(axis=0)\n",
    "    df.fillna(df_mean, inplace=True)\n",
    "\n",
    "del meta_train， train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "y_in_gal = full_train_in_gal['target']\n",
    "\n",
    "train_in_gal = full_train_in_gal.copy()\n",
    "\n",
    "del train_in_gal['target']\n",
    "\n",
    "# print('Training begins...')\n",
    "\n",
    "val_score_list = []\n",
    "clf_list = []\n",
    "\n",
    "ss_in, y_categorical_in_gal, oof_preds_in_gal, clf_in = train_by_nn(train_in_gal, y_in_gal)\n",
    "\n",
    "score_in_gal = multi_weighted_logloss(y_categorical_in_gal,oof_preds_in_gal)\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % score_in_gal)\n",
    "val_score_list.append(score_in_gal)\n",
    "clf_list.append(clf_in)\n",
    "\n",
    "\n",
    "y_out_gal = full_train_out_gal['target']\n",
    "\n",
    "train_out_gal = full_train_out_gal.copy()\n",
    "\n",
    "del train_out_gal['target']\n",
    "\n",
    "ss_out, y_categorical_out_gal, oof_preds_out_gal, clf_out = train_by_nn(train_out_gal, y_out_gal)\n",
    "\n",
    "score_out_gal = multi_weighted_logloss(y_categorical_out_gal,oof_preds_out_gal)\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % score_out_gal)\n",
    "val_score_list.append(score_out_gal)\n",
    "clf_list.append(clf_out)\n",
    "\n",
    "\n",
    "all_y = np.concatenate((y_in_gal.values, y_out_gal.values), axis=0)\n",
    "\n",
    "in_classes = [6, 16, 53, 65, 92]\n",
    "out_classes = [15, 42, 52, 62, 64, 67, 88, 90, 95]\n",
    "\n",
    "in_df = set_df(oof_preds_in_gal, in_classes)\n",
    "out_df = set_df(oof_preds_out_gal, out_classes)\n",
    "in_out_df = pd.concat([in_df, out_df], axis=0).fillna(0)\n",
    "\n",
    "\n",
    "print('Just double check:', multi_weighted_logloss_OLDVERSION(y_true=all_y, y_preds=in_out_df.values))\n",
    "\n",
    "all_y_cat = to_cat(all_y)\n",
    "\n",
    "tot_score = multi_weighted_logloss(all_y_cat, in_out_df.values)\n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % tot_score)\n",
    "\n",
    "val_score_list.append(tot_score)\n",
    "\n",
    "\n",
    "\n",
    "score_tab = pd.DataFrame({'Model':['Galaxy_Model', 'Extragalaxy_Model','Bi_Model'], 'Score':val_score_list})\n",
    "print(score_tab)\n",
    "score_tab.to_csv(r'../feat/validation_scores_{}.csv'.format(datetime.datetime.now().strftime('%m%d_%H%M')), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confusion_matrix(all_y, in_out_df.values)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
