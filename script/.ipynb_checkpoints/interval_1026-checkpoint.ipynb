{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger():\n",
    "    logger_ = logging.getLogger('main')\n",
    "    logger_.setLevel(logging.DEBUG)\n",
    "    fh = logging.FileHandler('simple_lightgbm.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s]%(asctime)s:%(name)s:%(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    # add the handlers to the logger\n",
    "    logger_.addHandler(fh)\n",
    "    logger_.addHandler(ch)\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    return logging.getLogger('main')\n",
    "\n",
    "\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "\n",
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def predict_chunk(df_, clfs_, meta_, features, train_mean):\n",
    "\n",
    "    df_['flux_ratio_sq'] = np.power(df_['flux'] / df_['flux_err'], 2.0)\n",
    "    df_['flux_by_flux_ratio_sq'] = df_['flux'] * df_['flux_ratio_sq']\n",
    "\n",
    "    # Group by object id\n",
    "    aggs = get_aggregations()\n",
    "\n",
    "    aggs = get_aggregations()\n",
    "    aggs['flux_ratio_sq'] = ['sum']\n",
    "    aggs['flux_by_flux_ratio_sq'] = ['sum']\n",
    "\n",
    "    new_columns = get_new_columns(aggs)\n",
    "\n",
    "    agg_ = df_.groupby('object_id').agg(aggs)\n",
    "    agg_.columns = new_columns\n",
    "\n",
    "    agg_ = add_features_to_agg(df=agg_)\n",
    "\n",
    "    # Merge with meta data\n",
    "    full_test = agg_.reset_index().merge(\n",
    "        right=meta_,\n",
    "        how='left',\n",
    "        on='object_id'\n",
    "    )\n",
    "\n",
    "    full_test = full_test.fillna(train_mean)\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "\n",
    "    print(preds_df_['class_99'].mean())\n",
    "\n",
    "    del agg_, full_test, preds_\n",
    "    gc.collect()\n",
    "\n",
    "    return preds_df_\n",
    "\n",
    "\n",
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    plt.figure(figsize=(8, 24))\n",
    "    sns.barplot(x='gain', y='feature', data=importances_.sort_values('mean_gain', ascending=False))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'../feat/importances_{}.png'.format(datetime.datetime.now().strftime('%m%d_%H%M')))\n",
    "    importances_.sort_values('mean_gain', ascending=False).to_csv(r'../feat/feat_rank_{}.csv'.format(datetime.datetime.now().strftime('%m%d_%H%M')), index=False)\n",
    "\n",
    "\n",
    "def train_classifiers(full_train=None, y=None):\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 14,\n",
    "        'metric': 'multi_logloss',\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': .9,\n",
    "        'colsample_bytree': .7,\n",
    "        'reg_alpha': .01,\n",
    "        'reg_lambda': .01,\n",
    "        'min_split_gain': 0.01,\n",
    "        'min_child_weight': 10,\n",
    "        'n_estimators': 1000,\n",
    "        'silent': -1,\n",
    "        'verbose': -1,\n",
    "        'max_depth': 3\n",
    "    }\n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**lgb_params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgb_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        get_logger().info(multi_weighted_logloss(val_y, clf.predict_proba(val_x, num_iteration=clf.best_iteration_)))\n",
    "\n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df['feature'] = full_train.columns\n",
    "        imp_df['gain'] = clf.feature_importances_\n",
    "        imp_df['fold'] = fold_ + 1\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "        clfs.append(clf)\n",
    "\n",
    "    get_logger().info('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))\n",
    "\n",
    "    return clfs, importances\n",
    "\n",
    "\n",
    "def get_aggregations():\n",
    "    return {\n",
    "        'mjd': ['min', 'max', 'size'],\n",
    "        'passband': ['mean', 'std', 'var'],  # ''min', 'max', 'mean', 'median', 'std'],\n",
    "#         'flux': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'detected': ['mean'],  # ''min', 'max', 'mean', 'median', 'std'],\n",
    "    }\n",
    "\n",
    "\n",
    "def get_new_columns(aggs):\n",
    "    return [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "\n",
    "def add_features_to_agg(df):\n",
    "    df['mjd_diff'] = df['mjd_max'] - df['mjd_min']\n",
    "    df['flux_diff'] = df['flux_max'] - df['flux_min']\n",
    "    df['flux_dif2'] = (df['flux_max'] - df['flux_min']) / df['flux_mean']\n",
    "    df['flux_w_mean'] = df['flux_by_flux_ratio_sq_sum'] / df['flux_ratio_sq_sum']\n",
    "    df['flux_dif3'] = (df['flux_max'] - df['flux_min']) / df['flux_w_mean']\n",
    "\n",
    "\n",
    "\n",
    "    del df['mjd_max'], df['mjd_min']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>mjd</th>\n",
       "      <th>passband</th>\n",
       "      <th>flux</th>\n",
       "      <th>flux_err</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4229</td>\n",
       "      <td>2</td>\n",
       "      <td>-544.810303</td>\n",
       "      <td>3.622952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4306</td>\n",
       "      <td>1</td>\n",
       "      <td>-816.434326</td>\n",
       "      <td>5.553370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4383</td>\n",
       "      <td>3</td>\n",
       "      <td>-471.385529</td>\n",
       "      <td>3.801213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4450</td>\n",
       "      <td>4</td>\n",
       "      <td>-388.984985</td>\n",
       "      <td>11.395031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>615</td>\n",
       "      <td>59752.4070</td>\n",
       "      <td>2</td>\n",
       "      <td>-681.858887</td>\n",
       "      <td>4.041204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id         mjd  passband        flux   flux_err  detected\n",
       "0        615  59750.4229         2 -544.810303   3.622952         1\n",
       "1        615  59750.4306         1 -816.434326   5.553370         1\n",
       "2        615  59750.4383         3 -471.385529   3.801213         1\n",
       "3        615  59750.4450         4 -388.984985  11.395031         1\n",
       "4        615  59752.4070         2 -681.858887   4.041204         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/training_set.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train = pd.read_csv('../input/training_set_metadata.csv')\n",
    "meta_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.passband.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# band = train.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "# band['object_id'] = band.index\n",
    "# band.reset_index(drop=True, inplace=True)\n",
    "# band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train[meta_train.distmod.isnull()]['mwebv'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train[meta_train.distmod.notnull()]['distmod'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train['distmod'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_band_feats(df, db):\n",
    "    \n",
    "    \n",
    "### 均值\n",
    "    print('Adding feats for the flux mean per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_mean' for col in stats.columns.tolist()]\n",
    "    \n",
    "    # band_#_flux_mean互相减去\n",
    "    mean_cols = stats.columns.tolist()\n",
    "    for col in mean_cols:\n",
    "        subtract_cols = [col_ for col_ in mean_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "      \n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "### 标准差    \n",
    "    print('Adding feats for the flux std per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].std().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_std' for col in stats.columns.tolist()]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "\n",
    "    \n",
    "### 偏度    \n",
    "    print('Adding feats for the flux skew per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].skew().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_skew' for col in stats.columns.tolist()]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "### 最大值\n",
    "    print('Adding feats for the flux max per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].max().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_max' for col in stats.columns.tolist()]\n",
    "    # band_#_flux_max互相减去\n",
    "    max_cols = stats.columns.tolist()\n",
    "    for col in max_cols:\n",
    "        subtract_cols = [col_ for col_ in max_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "            \n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "### 最小值    \n",
    "    print('Adding feats for the flux min per band...')\n",
    "    stats = db.groupby(['object_id','passband'])['flux'].min().unstack()\n",
    "    stats.columns = ['band_' + str(col) + '_flux_min' for col in stats.columns.tolist()]\n",
    "    # band_#_flux_min互相做差\n",
    "    min_cols = stats.columns.tolist()\n",
    "    for col in min_cols:\n",
    "        subtract_cols = [col_ for col_ in min_cols if col_ < col]\n",
    "        for sub_col in subtract_cols:\n",
    "            stats['{}_minus_{}'.format(col, sub_col)] = stats[col] - stats[sub_col]\n",
    "    print('Feats added:',stats.columns.tolist())\n",
    "    stats['object_id'] = stats.index    \n",
    "    df = df.merge(stats, on='object_id', how='left').fillna(0) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# ### ......  Mean\n",
    "#     print('Adding feats for the flux_err mean per band...')\n",
    "#     stats = db.groupby(['object_id','passband'])['flux_err'].mean().unstack()\n",
    "#     stats.columns = ['band_' + str(col) + '_flux_err_mean' for col in stats.columns.tolist()]      \n",
    "#     print('Feats added:',stats.columns.tolist())\n",
    "#     stats['object_id'] = stats.index    \n",
    "#     df = df.merge(stats, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### 遍历band_list计算 \n",
    "    print('Adding feats for the flux (max-min)/mean per band...')\n",
    "    for band_n in range(6):\n",
    "        df['band_' + str(band_n) + '_flux_diff1'] = df['band_' + str(band_n) + '_flux_max'] - df['band_' + str(band_n) + '_flux_min']\n",
    "        df['band_' + str(band_n) + '_flux_diff2'] = df['band_' + str(band_n) + '_flux_diff1']/df['band_' + str(band_n) + '_flux_mean']\n",
    "        print('Feature added: band_' + str(band_n) + '_flux_diff2')\n",
    "        \n",
    "#         df['band_' + str(band_n) + '_flux_err_ratio'] = df['band_' + str(band_n) + '_flux_err_mean']/df['band_' + str(band_n) + '_flux_mean']\n",
    "#         print('Feature added: band_' + str(band_n) + '_flux_err_ratio')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "#     print('Adding feats for the flux mean per band...')\n",
    "#     stats = db.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "#     stats['object_id'] = stats.index\n",
    "#     stats.columns = [str(col) + '_mean' for col in stats.columns.tolist()]\n",
    "#     df = df.merge(db, on='object_id', how='left').fillna(0)\n",
    "#     print('Feats added:',stats.columns.tolist())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv('../input/training_set.csv')\n",
    "    train['flux_ratio_sq'] = np.power(train['flux'] / train['flux_err'], 2.0)\n",
    "    train['flux_by_flux_ratio_sq'] = train['flux'] * train['flux_ratio_sq']\n",
    "    \n",
    "#     # 增加正负1sigma\n",
    "#     train['flux_m_err'] = train['flux'] - train['flux_err'] \n",
    "#     train['flux_p_err'] = train['flux'] + train['flux_err'] \n",
    "#     # 增加正负1sigma\n",
    "\n",
    "    # train = pd.concat([train, pd.get_dummies(train['passband'], prefix='passband')], axis=1, sort=False)\n",
    "\n",
    "    aggs = get_aggregations()\n",
    "    aggs['flux_ratio_sq'] = ['sum']\n",
    "    aggs['flux_by_flux_ratio_sq'] = ['sum']\n",
    "\n",
    "    \n",
    "#     # 增加正负1sigma    \n",
    "#     aggs['flux_m_err'] = ['sum']\n",
    "#     aggs['flux_p_err'] = ['sum']\n",
    "    \n",
    "#     aggs['flux_m_err'] = ['mean']\n",
    "#     aggs['flux_p_err'] = ['mean']\n",
    "#     # 增加正负1sigma\n",
    "    \n",
    "    \n",
    "    # passbands = [f for f in train if 'passband_' in f]\n",
    "    # get_logger().info('Passband features : {}'.format(passbands))\n",
    "    # for pb in passbands:\n",
    "    #     aggs[pb] = ['mean']\n",
    "\n",
    "    agg_train = train.groupby('object_id').agg(aggs)\n",
    "    new_columns = get_new_columns(aggs)\n",
    "    agg_train.columns = new_columns\n",
    "\n",
    "    agg_train = add_features_to_agg(df=agg_train)\n",
    "    \n",
    "    agg_train.head()\n",
    "\n",
    "#     del train\n",
    "#     gc.collect()\n",
    "\n",
    "    meta_train = pd.read_csv('../input/training_set_metadata.csv')\n",
    "    meta_train.head()\n",
    "    \n",
    "    # 增加是否在银河系的特征\n",
    "#     distmod_min = meta_train.distmod.min()\n",
    "#     meta_train.distmod.fillna(distmod_min,inplace=True)\n",
    "#     meta_train['in_MilkyWay'] = 0\n",
    "#     meta_train[meta_train.in_MilkyWay == 1]\n",
    "    # 增加是否在银河系的特征\n",
    "\n",
    "    full_train = agg_train.reset_index().merge(\n",
    "        right=meta_train,\n",
    "        how='outer',\n",
    "        on='object_id'\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    y = full_train['target']\n",
    "    del full_train['target']\n",
    "#     del full_train['object_id'], full_train['hostgal_specz']  # , full_train['distmod']\n",
    "    del full_train['hostgal_specz']  # , full_train['distmod']\n",
    "\n",
    "    train_mean = full_train.mean(axis=0)\n",
    "    full_train.fillna(train_mean, inplace=True)\n",
    "    \n",
    "    # 增加 mean_flux_per_band\n",
    "#     band = train.groupby(['object_id','passband'])['flux'].mean().unstack()\n",
    "#     band['object_id'] = band.index\n",
    "#     band.reset_index(drop=True, inplace=True)\n",
    "#     full_train = full_train.merge(band, on='object_id', how='left').fillna(0)\n",
    "    \n",
    "    full_train = add_band_feats(full_train, train)\n",
    "    \n",
    "\n",
    "    # 增加 mean_flux_per_band\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    del full_train['object_id']\n",
    "        \n",
    "    del train\n",
    "    gc.collect()\n",
    "\n",
    "        \n",
    "    get_logger().info(full_train.columns)\n",
    "    clfs, importances = train_classifiers(full_train, y)\n",
    "\n",
    "    save_importances(importances_=importances)\n",
    "\n",
    "#     meta_test = pd.read_csv('../input/test_set_metadata.csv')\n",
    "\n",
    "#     import time\n",
    "\n",
    "#     start = time.time()\n",
    "#     chunks = 5000000\n",
    "#     remain_df = None\n",
    "\n",
    "#     for i_c, df in enumerate(pd.read_csv('../input/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "#         # Check object_ids\n",
    "#         # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "#         unique_ids = np.unique(df['object_id'])\n",
    "#         new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "\n",
    "#         if remain_df is None:\n",
    "#             df = df.loc[df['object_id'].isin(unique_ids[:-1])].copy()\n",
    "#         else:\n",
    "#             df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "\n",
    "#         # Create remaining samples df\n",
    "#         remain_df = new_remain_df\n",
    "\n",
    "#         preds_df = predict_chunk(df_=df,\n",
    "#                                  clfs_=clfs,\n",
    "#                                  meta_=meta_test,\n",
    "#                                  features=full_train.columns,\n",
    "#                                  train_mean=train_mean)\n",
    "\n",
    "#         if i_c == 0:\n",
    "#             preds_df.to_csv('predictions_v3.csv', header=True, index=False, float_format='%.6f')\n",
    "#         else:\n",
    "#             preds_df.to_csv('predictions_v3.csv', header=False, mode='a', index=False, float_format='%.6f')\n",
    "\n",
    "#         del preds_df\n",
    "#         gc.collect()\n",
    "\n",
    "#         if (i_c + 1) % 10 == 0:\n",
    "#             get_logger().info('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "#             print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))\n",
    "\n",
    "#     # Compute last object in remain_df\n",
    "\n",
    "#     preds_df = predict_chunk(df_=remain_df,\n",
    "#                              clfs_=clfs,\n",
    "#                              meta_=meta_test,\n",
    "#                              features=full_train.columns,\n",
    "#                              train_mean=train_mean)\n",
    "\n",
    "#     preds_df.to_csv('predictions_v3.csv', header=False, mode='a', index=False, float_format='%.6f')\n",
    "\n",
    "#     z = pd.read_csv('predictions_v3.csv')\n",
    "\n",
    "#     z = z.groupby('object_id').mean()\n",
    "\n",
    "#     z.to_csv('single_predictions_v3.csv', index=True, float_format='%.6f')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "create_logger()\n",
    "try:\n",
    "    main()\n",
    "except Exception:\n",
    "    get_logger().exception('Unexpected Exception Occured')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
